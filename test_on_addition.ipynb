{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Transfer Models on Original Addition Task\n",
    "\n",
    "**Goal**: Test all models (source addition + 3 transferred subtraction models) on the **original addition task** to check:\n",
    "1. Do transferred models retain addition knowledge?\n",
    "2. Is there catastrophic forgetting during subtraction training?\n",
    "3. Do memorized/random models learn addition as a side effect?\n",
    "\n",
    "**Models to test**:\n",
    "- Grokked Addition (source) - should be 100%\n",
    "- Grokked Transfer - fine-tuned for subtraction, but did it forget addition?\n",
    "- Memorized Transfer - started from memorized addition, does it remember?\n",
    "- Random Baseline - never saw addition, should be random (~0.9%)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and navigate to repo\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "if not os.path.exists('progress-measures-paper-extension'):\n",
    "    !git clone https://github.com/Junekhunter/progress-measures-paper-extension.git\n",
    "\n",
    "os.chdir('progress-measures-paper-extension')\n",
    "!pip install -q einops\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import replace\n",
    "\n",
    "from transformers import Transformer, Config, gen_train_test\n",
    "import helpers\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EXPERIMENT_DIR = input(\"Enter your 3-way experiment directory path: \")\n",
    "SEED_TO_ANALYZE = 42  # Use seed 42 (typical, not outlier)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Analyzing seed: {SEED_TO_ANALYZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load All Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the source grokked addition model\n",
    "print(\"Loading grokked addition (source) model...\")\n",
    "addition_checkpoint = torch.load('saved_runs/wd_10-1_mod_addition_loss_curve.pth', map_location='cpu')\n",
    "\n",
    "addition_config = Config(\n",
    "    lr=1e-3,\n",
    "    weight_decay=1.0,\n",
    "    p=113,\n",
    "    d_model=128,\n",
    "    fn_name='add',\n",
    "    frac_train=0.3,\n",
    "    seed=0,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "grokked_addition_model = Transformer(addition_config, use_cache=False)\n",
    "if 'model' in addition_checkpoint:\n",
    "    grokked_addition_model.load_state_dict(addition_checkpoint['model'])\n",
    "else:\n",
    "    grokked_addition_model.load_state_dict(addition_checkpoint['state_dicts'][-1])\n",
    "grokked_addition_model.to(device)\n",
    "grokked_addition_model.eval()\n",
    "\n",
    "print(\"✓ Grokked addition model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the three subtraction models (which we'll test on addition)\n",
    "subtraction_config = replace(addition_config, fn_name='subtract', seed=SEED_TO_ANALYZE)\n",
    "\n",
    "models = {}\n",
    "\n",
    "for condition in ['grokked_transfer', 'memorized_transfer', 'random_baseline']:\n",
    "    print(f\"Loading {condition} model (seed {SEED_TO_ANALYZE})...\")\n",
    "    \n",
    "    checkpoint_path = f'{EXPERIMENT_DIR}/checkpoints/{condition}_seed{SEED_TO_ANALYZE}.pth'\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    # Create model with SUBTRACTION config (that's what it was trained on)\n",
    "    model = Transformer(subtraction_config, use_cache=False)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    models[condition] = {\n",
    "        'model': model,\n",
    "        'subtraction_accuracy': checkpoint['final_test_accuracy'],\n",
    "        'epochs_trained': checkpoint['threshold_epochs'].get(0.999, 'N/A')\n",
    "    }\n",
    "    \n",
    "    print(f\"  Subtraction accuracy: {checkpoint['final_test_accuracy']:.4f}\")\n",
    "\n",
    "print(\"\\n✓ All models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Generate Addition Test Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate addition train and test sets\n",
    "addition_train, addition_test = gen_train_test(addition_config)\n",
    "\n",
    "print(f\"Addition dataset:\")\n",
    "print(f\"  Train: {len(addition_train)} examples\")\n",
    "print(f\"  Test:  {len(addition_test)} examples\")\n",
    "\n",
    "# Convert to tensors\n",
    "addition_train_tensor = torch.tensor(addition_train).to(device)\n",
    "addition_test_tensor = torch.tensor(addition_test).to(device)\n",
    "\n",
    "print(\"✓ Addition datasets created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test All Models on Addition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_on_addition(model, data, config):\n",
    "    \"\"\"\n",
    "    Evaluate a model on addition task.\n",
    "    Returns accuracy on the addition dataset.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(data)\n",
    "        predictions = logits[:, -1, :config.p].argmax(dim=-1)\n",
    "    \n",
    "    # Calculate correct answers for addition\n",
    "    targets = torch.tensor([config.fn(a, b) for a, b, _ in data]).to(config.device)\n",
    "    \n",
    "    accuracy = (predictions == targets).float().mean().item()\n",
    "    \n",
    "    return accuracy\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TESTING ALL MODELS ON ADDITION TASK\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "results = {}\n",
    "\n",
    "# Test grokked addition (source) - should be ~100%\n",
    "print(\"\\n1. GROKKED ADDITION (Source Model):\")\n",
    "train_acc = evaluate_on_addition(grokked_addition_model, addition_train_tensor, addition_config)\n",
    "test_acc = evaluate_on_addition(grokked_addition_model, addition_test_tensor, addition_config)\n",
    "print(f\"   Train accuracy: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "print(f\"   Test accuracy:  {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "results['grokked_addition'] = {'train': train_acc, 'test': test_acc}\n",
    "\n",
    "# Test transferred models on addition\n",
    "model_names = {\n",
    "    'grokked_transfer': '2. GROKKED TRANSFER',\n",
    "    'memorized_transfer': '3. MEMORIZED TRANSFER',\n",
    "    'random_baseline': '4. RANDOM BASELINE'\n",
    "}\n",
    "\n",
    "for condition, display_name in model_names.items():\n",
    "    print(f\"\\n{display_name}:\")\n",
    "    model = models[condition]['model']\n",
    "    \n",
    "    train_acc = evaluate_on_addition(model, addition_train_tensor, addition_config)\n",
    "    test_acc = evaluate_on_addition(model, addition_test_tensor, addition_config)\n",
    "    \n",
    "    print(f\"   Train accuracy on addition: {train_acc:.4f} ({train_acc*100:.2f}%)\")\n",
    "    print(f\"   Test accuracy on addition:  {test_acc:.4f} ({test_acc*100:.2f}%)\")\n",
    "    print(f\"   (Trained for subtraction: {models[condition]['subtraction_accuracy']:.4f} accuracy)\")\n",
    "    \n",
    "    results[condition] = {'train': train_acc, 'test': test_acc}\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comparison visualization\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "model_labels = ['Grokked\\nAddition\\n(Source)', 'Grokked\\nTransfer', 'Memorized\\nTransfer', 'Random\\nBaseline']\n",
    "model_keys = ['grokked_addition', 'grokked_transfer', 'memorized_transfer', 'random_baseline']\n",
    "colors = ['green', 'blue', 'purple', 'orange']\n",
    "\n",
    "# Plot 1: Addition test accuracy\n",
    "ax = axes[0]\n",
    "test_accs = [results[k]['test'] for k in model_keys]\n",
    "bars = ax.bar(range(len(model_labels)), test_accs, color=colors, alpha=0.7, edgecolor='black')\n",
    "\n",
    "ax.set_ylabel('Accuracy on Addition', fontsize=12)\n",
    "ax.set_title('Addition Test Accuracy\\n(After Subtraction Training)', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(range(len(model_labels)))\n",
    "ax.set_xticklabels(model_labels, fontsize=10)\n",
    "ax.set_ylim([0, 1.05])\n",
    "ax.axhline(1.0, color='green', linestyle='--', alpha=0.5, label='Perfect')\n",
    "ax.axhline(1/113, color='red', linestyle='--', alpha=0.5, label='Random guess')\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "ax.legend()\n",
    "\n",
    "# Add value labels\n",
    "for i, (bar, acc) in enumerate(zip(bars, test_accs)):\n",
    "    height = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2., height,\n",
    "            f'{acc*100:.1f}%', ha='center', va='bottom', fontsize=11, fontweight='bold')\n",
    "\n",
    "# Plot 2: Train vs Test comparison\n",
    "ax = axes[1]\n",
    "x = np.arange(len(model_labels))\n",
    "width = 0.35\n",
    "\n",
    "train_accs = [results[k]['train'] for k in model_keys]\n",
    "test_accs = [results[k]['test'] for k in model_keys]\n",
    "\n",
    "bars1 = ax.bar(x - width/2, train_accs, width, label='Train', alpha=0.7, color='lightblue', edgecolor='black')\n",
    "bars2 = ax.bar(x + width/2, test_accs, width, label='Test', alpha=0.7, color='darkblue', edgecolor='black')\n",
    "\n",
    "ax.set_ylabel('Accuracy on Addition', fontsize=12)\n",
    "ax.set_title('Train vs Test Accuracy on Addition', fontsize=14, fontweight='bold')\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(model_labels, fontsize=10)\n",
    "ax.set_ylim([0, 1.05])\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{EXPERIMENT_DIR}/figures/addition_retention_test.png', dpi=200, bbox_inches='tight')\n",
    "print(\"✓ Saved: addition_retention_test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Analysis and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS: ADDITION RETENTION AFTER SUBTRACTION TRAINING\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Calculate retention rates\n",
    "grok_retention = results['grokked_transfer']['test']\n",
    "mem_retention = results['memorized_transfer']['test']\n",
    "rand_retention = results['random_baseline']['test']\n",
    "\n",
    "print(\"\\n1. CATASTROPHIC FORGETTING:\")\n",
    "print(\"-\" * 80)\n",
    "print(f\"   Grokked Transfer:   {grok_retention*100:.2f}% addition accuracy retained\")\n",
    "print(f\"   Memorized Transfer: {mem_retention*100:.2f}% addition accuracy retained\")\n",
    "print(f\"   Random Baseline:    {rand_retention*100:.2f}% (never learned addition)\")\n",
    "\n",
    "if grok_retention > 0.9:\n",
    "    print(\"\\n   → Grokked model shows STRONG RETENTION (>90%)\")\n",
    "    print(\"     The generalized circuit is robust to fine-tuning!\")\n",
    "elif grok_retention > 0.5:\n",
    "    print(\"\\n   → Grokked model shows PARTIAL RETENTION (50-90%)\")\n",
    "    print(\"     Some forgetting occurred during subtraction training\")\n",
    "else:\n",
    "    print(\"\\n   → Grokked model shows CATASTROPHIC FORGETTING (<50%)\")\n",
    "    print(\"     Addition circuit was overwritten by subtraction\")\n",
    "\n",
    "if mem_retention > 0.5:\n",
    "    print(\"\\n   → Memorized model retained some addition knowledge\")\n",
    "else:\n",
    "    print(\"\\n   → Memorized model forgot addition (as expected from memorization)\")\n",
    "\n",
    "print(\"\\n2. COMPARISON TO SUBTRACTION PERFORMANCE:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "for condition in ['grokked_transfer', 'memorized_transfer', 'random_baseline']:\n",
    "    add_acc = results[condition]['test']\n",
    "    sub_acc = models[condition]['subtraction_accuracy']\n",
    "    \n",
    "    print(f\"\\n   {condition.replace('_', ' ').title()}:\")\n",
    "    print(f\"      Addition (original):  {add_acc*100:.2f}%\")\n",
    "    print(f\"      Subtraction (target): {sub_acc*100:.2f}%\")\n",
    "    \n",
    "    if add_acc > 0.9 and sub_acc > 0.9:\n",
    "        print(f\"      → Multi-task success: Can do BOTH tasks!\")\n",
    "    elif sub_acc > 0.9:\n",
    "        print(f\"      → Specialized for subtraction, forgot addition\")\n",
    "    else:\n",
    "        print(f\"      → Poor performance on both tasks\")\n",
    "\n",
    "print(\"\\n3. KEY INSIGHTS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if grok_retention > mem_retention:\n",
    "    ratio = grok_retention / (mem_retention + 1e-6)\n",
    "    print(f\"\\n   ✓ Grokked models retain {ratio:.1f}x more addition knowledge\")\n",
    "    print(\"     Generalized circuits are more robust to catastrophic forgetting!\")\n",
    "else:\n",
    "    print(\"\\n   ⚠ Memorized models retained more addition knowledge\")\n",
    "    print(\"     Unexpected - may indicate task similarity or overfitting\")\n",
    "\n",
    "if grok_retention > 0.9 and mem_retention < 0.5:\n",
    "    print(\"\\n   ✓ This demonstrates POLYSEMANTICITY in grokked circuits:\")\n",
    "    print(\"     The same frequency-based circuit can solve BOTH tasks!\")\n",
    "    print(\"     Addition and subtraction share the same underlying structure.\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
