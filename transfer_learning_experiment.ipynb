{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning: Grokked Addition → Subtraction\n",
    "\n",
    "This notebook tests whether a grokked modular addition model can transfer to accelerate learning on modular subtraction.\n",
    "\n",
    "**Experiment Plan:**\n",
    "1. Load a fully grokked addition model (mod 113)\n",
    "2. Fine-tune it on subtraction task (a - b mod 113)\n",
    "3. Compare against random initialization baseline\n",
    "\n",
    "**Key Metrics:**\n",
    "- Epochs to reach 90% test accuracy\n",
    "- Test accuracy curves over training\n",
    "- Training loss curves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup: Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository if not already cloned\n",
    "import os\n",
    "if not os.path.exists('progress-measures-paper-extension'):\n",
    "    !git clone https://github.com/Junekhunter/progress-measures-paper-extension.git\n",
    "    \n",
    "# Change to repo directory\n",
    "os.chdir('progress-measures-paper-extension')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install any missing dependencies (Colab has most already)\n",
    "!pip install -q einops wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass, replace\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Import from the repo\n",
    "from transformers import Transformer, Config, gen_train_test, full_loss\n",
    "import helpers\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Inspect and Load Grokked Addition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint and inspect\n",
    "checkpoint_path = 'saved_runs/wd_10-1_mod_addition_loss_curve.pth'\n",
    "print(f\"Loading checkpoint from {checkpoint_path}...\")\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "print(f\"\\nCheckpoint keys: {list(checkpoint.keys())}\")\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "\n",
    "# Analyze the checkpoint structure\n",
    "if 'config' in checkpoint:\n",
    "    print(f\"\\nConfig: {checkpoint['config']}\")\n",
    "    \n",
    "if 'test_losses' in checkpoint:\n",
    "    test_losses = checkpoint['test_losses']\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    \n",
    "    print(f\"\\nTotal training epochs: {len(test_losses)}\")\n",
    "    print(f\"\\nFinal 10 epochs:\")\n",
    "    for i in range(max(0, len(test_losses)-10), len(test_losses)):\n",
    "        print(f\"  Epoch {i}: train_loss={train_losses[i]:.6f}, test_loss={test_losses[i]:.6f}\")\n",
    "    \n",
    "    # Check if model is fully grokked\n",
    "    final_test_loss = test_losses[-1]\n",
    "    if final_test_loss < 0.01:\n",
    "        print(f\"\\n✓ Model is FULLY GROKKED (final test loss: {final_test_loss:.6f})\")\n",
    "    else:\n",
    "        print(f\"\\n✗ Model NOT fully grokked (final test loss: {final_test_loss:.6f})\")\n",
    "        \n",
    "# Plot training curves\n",
    "if 'train_losses' in checkpoint and 'test_losses' in checkpoint:\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(checkpoint['train_losses'], label='Train Loss', alpha=0.7)\n",
    "    plt.plot(checkpoint['test_losses'], label='Test Loss', alpha=0.7)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Grokked Addition Model: Training Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(np.log10(np.array(checkpoint['train_losses'])+1e-10), label='Log Train Loss', alpha=0.7)\n",
    "    plt.plot(np.log10(np.array(checkpoint['test_losses'])+1e-10), label='Log Test Loss', alpha=0.7)\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Log10(Loss)')\n",
    "    plt.title('Grokked Addition Model: Log Training Curves')\n",
    "    plt.legend()\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config for the addition model\n",
    "addition_config = Config(\n",
    "    lr=1e-3,\n",
    "    weight_decay=1.0,\n",
    "    p=113,\n",
    "    d_model=128,\n",
    "    fn_name='add',\n",
    "    frac_train=0.3,\n",
    "    num_epochs=50000,\n",
    "    seed=0,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")\n",
    "\n",
    "# Create model and load grokked weights\n",
    "grokked_addition_model = Transformer(addition_config, use_cache=False)\n",
    "grokked_addition_model.to(addition_config.device)\n",
    "\n",
    "# Load the trained weights\n",
    "if 'model' in checkpoint:\n",
    "    grokked_addition_model.load_state_dict(checkpoint['model'])\n",
    "    print(\"✓ Loaded model from 'model' key\")\n",
    "elif 'state_dicts' in checkpoint:\n",
    "    # If there are multiple checkpoints, use the last one\n",
    "    grokked_addition_model.load_state_dict(checkpoint['state_dicts'][-1])\n",
    "    print(f\"✓ Loaded model from 'state_dicts' (checkpoint {len(checkpoint['state_dicts'])-1})\")\n",
    "else:\n",
    "    print(\"✗ Could not find model weights in checkpoint!\")\n",
    "\n",
    "print(\"\\n✓ Grokked addition model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Verify Addition Model Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the loaded model on addition\n",
    "grokked_addition_model.eval()\n",
    "\n",
    "# Generate test data\n",
    "test_samples = 20\n",
    "print(\"Testing grokked addition model on random examples:\\n\")\n",
    "print(\"Input (a, b) | Ground Truth (a+b mod 113) | Model Prediction\")\n",
    "print(\"-\" * 60)\n",
    "\n",
    "correct = 0\n",
    "with torch.no_grad():\n",
    "    for _ in range(test_samples):\n",
    "        a = np.random.randint(0, 113)\n",
    "        b = np.random.randint(0, 113)\n",
    "        ground_truth = (a + b) % 113\n",
    "        \n",
    "        # Prepare input\n",
    "        input_tensor = torch.tensor([[a, b, 113]]).to(addition_config.device)\n",
    "        logits = grokked_addition_model(input_tensor)[0, -1]\n",
    "        prediction = logits.argmax().item()\n",
    "        \n",
    "        is_correct = prediction == ground_truth\n",
    "        correct += is_correct\n",
    "        \n",
    "        symbol = \"✓\" if is_correct else \"✗\"\n",
    "        print(f\"{symbol} ({a:3d}, {b:3d}) | {ground_truth:3d} | {prediction:3d}\")\n",
    "\n",
    "accuracy = correct / test_samples * 100\n",
    "print(f\"\\nAccuracy: {accuracy:.1f}% ({correct}/{test_samples})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Define Training Function for Subtraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_subtraction_model(model, config, num_epochs=5000, save_every=100, verbose=True):\n",
    "    \"\"\"\n",
    "    Train a model on the subtraction task.\n",
    "    \n",
    "    Args:\n",
    "        model: Transformer model to train\n",
    "        config: Config object with fn_name='subtract'\n",
    "        num_epochs: Number of training epochs\n",
    "        save_every: How often to save metrics\n",
    "        verbose: Whether to print progress\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with train_losses, test_losses, and other metrics\n",
    "    \"\"\"\n",
    "    # Set up training\n",
    "    model.to(config.device)\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay, betas=(0.9, 0.98))\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(step/10, 1))\n",
    "    \n",
    "    # Generate train/test split\n",
    "    train_data, test_data = gen_train_test(config)\n",
    "    \n",
    "    # Tracking metrics\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    epochs_to_90_percent = None\n",
    "    \n",
    "    if verbose:\n",
    "        print(f\"Training on {len(train_data)} examples, testing on {len(test_data)} examples\")\n",
    "        pbar = tqdm(range(num_epochs), desc=\"Training\")\n",
    "    else:\n",
    "        pbar = range(num_epochs)\n",
    "    \n",
    "    for epoch in pbar:\n",
    "        # Training step\n",
    "        train_loss = full_loss(config, model, train_data)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Evaluation\n",
    "        with torch.no_grad():\n",
    "            test_loss = full_loss(config, model, test_data)\n",
    "            \n",
    "            # Calculate test accuracy\n",
    "            test_tensor = torch.tensor(test_data).to(config.device)\n",
    "            logits = model(test_tensor)[:, -1]\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            labels = torch.tensor([config.fn(i, j) for i, j, _ in test_data]).to(config.device)\n",
    "            test_accuracy = (predictions == labels).float().mean().item()\n",
    "        \n",
    "        train_losses.append(train_loss.item())\n",
    "        test_losses.append(test_loss.item())\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "        # Check if we reached 90% accuracy\n",
    "        if epochs_to_90_percent is None and test_accuracy >= 0.90:\n",
    "            epochs_to_90_percent = epoch\n",
    "            if verbose:\n",
    "                print(f\"\\n✓ Reached 90% test accuracy at epoch {epoch}\")\n",
    "        \n",
    "        # Update progress bar\n",
    "        if verbose and epoch % save_every == 0:\n",
    "            pbar.set_postfix({\n",
    "                'train_loss': f'{train_loss.item():.4f}',\n",
    "                'test_loss': f'{test_loss.item():.4f}',\n",
    "                'test_acc': f'{test_accuracy:.3f}'\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses,\n",
    "        'test_accuracies': test_accuracies,\n",
    "        'epochs_to_90_percent': epochs_to_90_percent,\n",
    "        'final_test_accuracy': test_accuracies[-1],\n",
    "        'model_state': model.state_dict()\n",
    "    }\n",
    "\n",
    "print(\"Training function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Transfer Learning Experiment (Grokked Addition → Subtraction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create config for subtraction task\n",
    "subtraction_config = replace(\n",
    "    addition_config,\n",
    "    fn_name='subtract',\n",
    "    seed=42  # Different seed for different train/test split\n",
    ")\n",
    "\n",
    "print(\"Starting Transfer Learning Experiment (Addition → Subtraction)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a model initialized with grokked addition weights\n",
    "transfer_model = Transformer(subtraction_config, use_cache=False)\n",
    "transfer_model.load_state_dict(grokked_addition_model.state_dict())\n",
    "transfer_model.to(subtraction_config.device)\n",
    "\n",
    "print(\"✓ Transfer model created with grokked addition weights\")\n",
    "\n",
    "# Train on subtraction\n",
    "transfer_results = train_subtraction_model(\n",
    "    transfer_model,\n",
    "    subtraction_config,\n",
    "    num_epochs=5000,\n",
    "    save_every=100,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Transfer Learning Results:\")\n",
    "print(f\"  Final test accuracy: {transfer_results['final_test_accuracy']:.4f}\")\n",
    "print(f\"  Epochs to 90% accuracy: {transfer_results['epochs_to_90_percent']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Baseline Experiment (Random Initialization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting Baseline Experiment (Random Initialization)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Create a fresh model with random initialization\n",
    "baseline_model = Transformer(subtraction_config, use_cache=False)\n",
    "baseline_model.to(subtraction_config.device)\n",
    "\n",
    "print(\"✓ Baseline model created with random initialization\")\n",
    "\n",
    "# Train on subtraction\n",
    "baseline_results = train_subtraction_model(\n",
    "    baseline_model,\n",
    "    subtraction_config,\n",
    "    num_epochs=5000,\n",
    "    save_every=100,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"Baseline Results:\")\n",
    "print(f\"  Final test accuracy: {baseline_results['final_test_accuracy']:.4f}\")\n",
    "print(f\"  Epochs to 90% accuracy: {baseline_results['epochs_to_90_percent']}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Summary comparison\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. Transfer Learning (Grokked Addition → Subtraction):\")\n",
    "print(f\"   - Final test accuracy: {transfer_results['final_test_accuracy']:.4f}\")\n",
    "print(f\"   - Epochs to 90% accuracy: {transfer_results['epochs_to_90_percent']}\")\n",
    "print(f\"   - Final train loss: {transfer_results['train_losses'][-1]:.6f}\")\n",
    "print(f\"   - Final test loss: {transfer_results['test_losses'][-1]:.6f}\")\n",
    "\n",
    "print(\"\\n2. Baseline (Random Initialization):\")\n",
    "print(f\"   - Final test accuracy: {baseline_results['final_test_accuracy']:.4f}\")\n",
    "print(f\"   - Epochs to 90% accuracy: {baseline_results['epochs_to_90_percent']}\")\n",
    "print(f\"   - Final train loss: {baseline_results['train_losses'][-1]:.6f}\")\n",
    "print(f\"   - Final test loss: {baseline_results['test_losses'][-1]:.6f}\")\n",
    "\n",
    "# Calculate speedup\n",
    "if transfer_results['epochs_to_90_percent'] and baseline_results['epochs_to_90_percent']:\n",
    "    speedup = baseline_results['epochs_to_90_percent'] / transfer_results['epochs_to_90_percent']\n",
    "    improvement = baseline_results['epochs_to_90_percent'] - transfer_results['epochs_to_90_percent']\n",
    "    print(\"\\n3. Transfer Learning Benefits:\")\n",
    "    print(f\"   - Speedup: {speedup:.2f}x faster to reach 90% accuracy\")\n",
    "    print(f\"   - Saved {improvement} epochs ({improvement/baseline_results['epochs_to_90_percent']*100:.1f}% reduction)\")\n",
    "else:\n",
    "    print(\"\\n3. Note: One or both models did not reach 90% accuracy within the training budget.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "\n",
    "# Plot 1: Training Loss\n",
    "axes[0, 0].plot(transfer_results['train_losses'], label='Transfer Learning', alpha=0.7, linewidth=2)\n",
    "axes[0, 0].plot(baseline_results['train_losses'], label='Random Init', alpha=0.7, linewidth=2)\n",
    "axes[0, 0].set_xlabel('Epoch')\n",
    "axes[0, 0].set_ylabel('Training Loss')\n",
    "axes[0, 0].set_title('Training Loss Over Time')\n",
    "axes[0, 0].legend()\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Test Loss\n",
    "axes[0, 1].plot(transfer_results['test_losses'], label='Transfer Learning', alpha=0.7, linewidth=2)\n",
    "axes[0, 1].plot(baseline_results['test_losses'], label='Random Init', alpha=0.7, linewidth=2)\n",
    "axes[0, 1].set_xlabel('Epoch')\n",
    "axes[0, 1].set_ylabel('Test Loss')\n",
    "axes[0, 1].set_title('Test Loss Over Time')\n",
    "axes[0, 1].legend()\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Test Accuracy\n",
    "axes[0, 2].plot(transfer_results['test_accuracies'], label='Transfer Learning', alpha=0.7, linewidth=2)\n",
    "axes[0, 2].plot(baseline_results['test_accuracies'], label='Random Init', alpha=0.7, linewidth=2)\n",
    "axes[0, 2].axhline(y=0.9, color='r', linestyle='--', alpha=0.5, label='90% Target')\n",
    "axes[0, 2].set_xlabel('Epoch')\n",
    "axes[0, 2].set_ylabel('Test Accuracy')\n",
    "axes[0, 2].set_title('Test Accuracy Over Time')\n",
    "axes[0, 2].legend()\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Log Training Loss\n",
    "axes[1, 0].plot(np.log10(np.array(transfer_results['train_losses'])+1e-10), label='Transfer Learning', alpha=0.7, linewidth=2)\n",
    "axes[1, 0].plot(np.log10(np.array(baseline_results['train_losses'])+1e-10), label='Random Init', alpha=0.7, linewidth=2)\n",
    "axes[1, 0].set_xlabel('Epoch')\n",
    "axes[1, 0].set_ylabel('Log10(Training Loss)')\n",
    "axes[1, 0].set_title('Log Training Loss Over Time')\n",
    "axes[1, 0].legend()\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Log Test Loss\n",
    "axes[1, 1].plot(np.log10(np.array(transfer_results['test_losses'])+1e-10), label='Transfer Learning', alpha=0.7, linewidth=2)\n",
    "axes[1, 1].plot(np.log10(np.array(baseline_results['test_losses'])+1e-10), label='Random Init', alpha=0.7, linewidth=2)\n",
    "axes[1, 1].set_xlabel('Epoch')\n",
    "axes[1, 1].set_ylabel('Log10(Test Loss)')\n",
    "axes[1, 1].set_title('Log Test Loss Over Time')\n",
    "axes[1, 1].legend()\n",
    "axes[1, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 6: First 1000 epochs (zoomed)\n",
    "zoom_epochs = 1000\n",
    "axes[1, 2].plot(transfer_results['test_accuracies'][:zoom_epochs], label='Transfer Learning', alpha=0.7, linewidth=2)\n",
    "axes[1, 2].plot(baseline_results['test_accuracies'][:zoom_epochs], label='Random Init', alpha=0.7, linewidth=2)\n",
    "axes[1, 2].axhline(y=0.9, color='r', linestyle='--', alpha=0.5, label='90% Target')\n",
    "\n",
    "# Mark the 90% achievement points\n",
    "if transfer_results['epochs_to_90_percent'] and transfer_results['epochs_to_90_percent'] < zoom_epochs:\n",
    "    axes[1, 2].axvline(x=transfer_results['epochs_to_90_percent'], color='blue', linestyle=':', alpha=0.5)\n",
    "    axes[1, 2].text(transfer_results['epochs_to_90_percent'], 0.85, f\"{transfer_results['epochs_to_90_percent']}\", \n",
    "                   rotation=90, verticalalignment='bottom', color='blue', fontsize=9)\n",
    "\n",
    "if baseline_results['epochs_to_90_percent'] and baseline_results['epochs_to_90_percent'] < zoom_epochs:\n",
    "    axes[1, 2].axvline(x=baseline_results['epochs_to_90_percent'], color='orange', linestyle=':', alpha=0.5)\n",
    "    axes[1, 2].text(baseline_results['epochs_to_90_percent'], 0.85, f\"{baseline_results['epochs_to_90_percent']}\", \n",
    "                   rotation=90, verticalalignment='bottom', color='orange', fontsize=9)\n",
    "\n",
    "axes[1, 2].set_xlabel('Epoch')\n",
    "axes[1, 2].set_ylabel('Test Accuracy')\n",
    "axes[1, 2].set_title(f'Test Accuracy (First {zoom_epochs} Epochs)')\n",
    "axes[1, 2].legend()\n",
    "axes[1, 2].grid(True, alpha=0.3)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('transfer_learning_results.png', dpi=150, bbox_inches='tight')\n",
    "print(\"✓ Saved visualization to transfer_learning_results.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save experiment results\n",
    "results_dict = {\n",
    "    'transfer_learning': transfer_results,\n",
    "    'baseline': baseline_results,\n",
    "    'experiment_config': {\n",
    "        'num_epochs': 5000,\n",
    "        'p': 113,\n",
    "        'frac_train': 0.3,\n",
    "        'lr': 1e-3,\n",
    "        'weight_decay': 1.0,\n",
    "        'source_checkpoint': checkpoint_path\n",
    "    }\n",
    "}\n",
    "\n",
    "torch.save(results_dict, 'transfer_learning_experiment_results.pth')\n",
    "print(\"✓ Saved results to transfer_learning_experiment_results.pth\")\n",
    "\n",
    "# Also save as numpy for easier analysis\n",
    "np.savez('transfer_learning_experiment_results.npz',\n",
    "         transfer_train_losses=np.array(transfer_results['train_losses']),\n",
    "         transfer_test_losses=np.array(transfer_results['test_losses']),\n",
    "         transfer_test_accuracies=np.array(transfer_results['test_accuracies']),\n",
    "         baseline_train_losses=np.array(baseline_results['train_losses']),\n",
    "         baseline_test_losses=np.array(baseline_results['test_losses']),\n",
    "         baseline_test_accuracies=np.array(baseline_results['test_accuracies']))\n",
    "print(\"✓ Saved results to transfer_learning_experiment_results.npz\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"EXPERIMENT COMPLETE!\")\n",
    "print(\"=\"*80)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook investigated whether a grokked modular addition model can transfer to accelerate learning on modular subtraction.\n",
    "\n",
    "**Key Findings:**\n",
    "- The transfer learning approach (grokked addition → subtraction) was compared against random initialization\n",
    "- Metrics tracked: epochs to 90% accuracy, test accuracy curves, and training loss curves\n",
    "- Results are visualized and saved for further analysis\n",
    "\n",
    "**Next Steps:**\n",
    "- Try different hyperparameters (learning rate, weight decay)\n",
    "- Test transfer to other operations (multiplication, division)\n",
    "- Analyze which model components transfer most effectively\n",
    "- Investigate if partially grokked models also show transfer benefits"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
