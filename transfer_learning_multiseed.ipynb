{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transfer Learning: Grokked Addition â†’ Subtraction (Multi-Seed)\n",
    "\n",
    "This notebook tests whether a grokked modular addition model can transfer to accelerate learning on modular subtraction.\n",
    "\n",
    "**Enhancements:**\n",
    "- âœ… Google Drive integration for persistent storage\n",
    "- âœ… Multiple seeds for statistical robustness\n",
    "- âœ… Aggregated results with mean/std/confidence intervals\n",
    "- âœ… Comprehensive visualizations\n",
    "\n",
    "**Experiment Plan:**\n",
    "1. Load a fully grokked addition model (mod 113)\n",
    "2. Run transfer learning experiments across N seeds\n",
    "3. Run baseline experiments across N seeds\n",
    "4. Aggregate and compare results statistically"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 0: Mount Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "import os\n",
    "from datetime import datetime\n",
    "\n",
    "# Mount Google Drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# Create experiment directory with timestamp\n",
    "timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')\n",
    "DRIVE_BASE = '/content/drive/MyDrive/grokking_transfer_experiments'\n",
    "EXPERIMENT_DIR = f'{DRIVE_BASE}/run_{timestamp}'\n",
    "\n",
    "os.makedirs(EXPERIMENT_DIR, exist_ok=True)\n",
    "print(f\"âœ“ Google Drive mounted\")\n",
    "print(f\"âœ“ Experiment directory: {EXPERIMENT_DIR}\")\n",
    "\n",
    "# Create subdirectories\n",
    "os.makedirs(f'{EXPERIMENT_DIR}/figures', exist_ok=True)\n",
    "os.makedirs(f'{EXPERIMENT_DIR}/checkpoints', exist_ok=True)\n",
    "os.makedirs(f'{EXPERIMENT_DIR}/results', exist_ok=True)\n",
    "print(\"âœ“ Created subdirectories: figures, checkpoints, results\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Setup - Clone Repository and Install Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clone the repository if not already cloned\n",
    "if not os.path.exists('progress-measures-paper-extension'):\n",
    "    !git clone https://github.com/Junekhunter/progress-measures-paper-extension.git\n",
    "    \n",
    "# Change to repo directory\n",
    "os.chdir('progress-measures-paper-extension')\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install any missing dependencies (Colab has most already)\n",
    "!pip install -q einops wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary modules\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from dataclasses import dataclass, replace\n",
    "import random\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "from collections import defaultdict\n",
    "\n",
    "# Import from the repo\n",
    "from transformers import Transformer, Config, gen_train_test, full_loss\n",
    "import helpers\n",
    "\n",
    "print(\"All imports successful!\")\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "    print(f\"CUDA device: {torch.cuda.get_device_name(0)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiment configuration\n",
    "NUM_SEEDS = 5  # Number of random seeds to run\n",
    "NUM_EPOCHS = 5000  # Training epochs per experiment\n",
    "CHECKPOINT_PATH = 'saved_runs/wd_10-1_mod_addition_loss_curve.pth'\n",
    "\n",
    "# Seeds for reproducibility\n",
    "SEEDS = [42, 123, 456, 789, 1024]\n",
    "assert len(SEEDS) == NUM_SEEDS\n",
    "\n",
    "print(f\"Experiment Configuration:\")\n",
    "print(f\"  - Number of seeds: {NUM_SEEDS}\")\n",
    "print(f\"  - Seeds: {SEEDS}\")\n",
    "print(f\"  - Epochs per run: {NUM_EPOCHS}\")\n",
    "print(f\"  - Source checkpoint: {CHECKPOINT_PATH}\")\n",
    "print(f\"  - Results will be saved to: {EXPERIMENT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Load Grokked Addition Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load checkpoint and inspect\n",
    "print(f\"Loading checkpoint from {CHECKPOINT_PATH}...\")\n",
    "checkpoint = torch.load(CHECKPOINT_PATH, map_location='cpu')\n",
    "\n",
    "print(f\"\\nCheckpoint keys: {list(checkpoint.keys())}\")\n",
    "\n",
    "# Analyze the checkpoint\n",
    "if 'test_losses' in checkpoint:\n",
    "    test_losses = checkpoint['test_losses']\n",
    "    train_losses = checkpoint['train_losses']\n",
    "    \n",
    "    print(f\"Total training epochs: {len(test_losses)}\")\n",
    "    \n",
    "    # Check if model is fully grokked\n",
    "    final_test_loss = test_losses[-1]\n",
    "    if final_test_loss < 0.01:\n",
    "        print(f\"âœ“ Model is FULLY GROKKED (final test loss: {final_test_loss:.6f})\")\n",
    "    else:\n",
    "        print(f\"âœ— Model NOT fully grokked (final test loss: {final_test_loss:.6f})\")\n",
    "\n",
    "# Create config for the addition model\n",
    "addition_config = Config(\n",
    "    lr=1e-3,\n",
    "    weight_decay=1.0,\n",
    "    p=113,\n",
    "    d_model=128,\n",
    "    fn_name='add',\n",
    "    frac_train=0.3,\n",
    "    num_epochs=50000,\n",
    "    seed=0,\n",
    "    device=torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    ")\n",
    "\n",
    "# Create model and load grokked weights\n",
    "grokked_addition_model = Transformer(addition_config, use_cache=False)\n",
    "grokked_addition_model.to(addition_config.device)\n",
    "\n",
    "# Load the trained weights\n",
    "if 'model' in checkpoint:\n",
    "    grokked_addition_model.load_state_dict(checkpoint['model'])\n",
    "    print(\"âœ“ Loaded model from 'model' key\")\n",
    "elif 'state_dicts' in checkpoint:\n",
    "    grokked_addition_model.load_state_dict(checkpoint['state_dicts'][-1])\n",
    "    print(f\"âœ“ Loaded model from 'state_dicts' (checkpoint {len(checkpoint['state_dicts'])-1})\")\n",
    "else:\n",
    "    print(\"âœ— Could not find model weights in checkpoint!\")\n",
    "\n",
    "print(\"\\nâœ“ Grokked addition model loaded successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Training Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_subtraction_model(model, config, num_epochs=5000, verbose=True, seed_label=\"\"):\n",
    "    \"\"\"\n",
    "    Train a model on the subtraction task.\n",
    "    \n",
    "    Args:\n",
    "        model: Transformer model to train\n",
    "        config: Config object with fn_name='subtract'\n",
    "        num_epochs: Number of training epochs\n",
    "        verbose: Whether to print progress\n",
    "        seed_label: Label for progress bar\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with train_losses, test_losses, and other metrics\n",
    "    \"\"\"\n",
    "    # Set up training\n",
    "    model.to(config.device)\n",
    "    model.train()\n",
    "    \n",
    "    optimizer = optim.AdamW(model.parameters(), lr=config.lr, weight_decay=config.weight_decay, betas=(0.9, 0.98))\n",
    "    scheduler = optim.lr_scheduler.LambdaLR(optimizer, lambda step: min(step/10, 1))\n",
    "    \n",
    "    # Generate train/test split\n",
    "    train_data, test_data = gen_train_test(config)\n",
    "    \n",
    "    # Tracking metrics\n",
    "    train_losses = []\n",
    "    test_losses = []\n",
    "    test_accuracies = []\n",
    "    epochs_to_90_percent = None\n",
    "    epochs_to_95_percent = None\n",
    "    epochs_to_99_percent = None\n",
    "    \n",
    "    desc = f\"Training {seed_label}\"\n",
    "    pbar = tqdm(range(num_epochs), desc=desc, disable=not verbose)\n",
    "    \n",
    "    for epoch in pbar:\n",
    "        # Training step\n",
    "        train_loss = full_loss(config, model, train_data)\n",
    "        train_loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Evaluation\n",
    "        with torch.no_grad():\n",
    "            test_loss = full_loss(config, model, test_data)\n",
    "            \n",
    "            # Calculate test accuracy\n",
    "            test_tensor = torch.tensor(test_data).to(config.device)\n",
    "            logits = model(test_tensor)[:, -1]\n",
    "            predictions = logits.argmax(dim=-1)\n",
    "            labels = torch.tensor([config.fn(i, j) for i, j, _ in test_data]).to(config.device)\n",
    "            test_accuracy = (predictions == labels).float().mean().item()\n",
    "        \n",
    "        train_losses.append(train_loss.item())\n",
    "        test_losses.append(test_loss.item())\n",
    "        test_accuracies.append(test_accuracy)\n",
    "        \n",
    "        # Track milestone epochs\n",
    "        if epochs_to_90_percent is None and test_accuracy >= 0.90:\n",
    "            epochs_to_90_percent = epoch\n",
    "        if epochs_to_95_percent is None and test_accuracy >= 0.95:\n",
    "            epochs_to_95_percent = epoch\n",
    "        if epochs_to_99_percent is None and test_accuracy >= 0.99:\n",
    "            epochs_to_99_percent = epoch\n",
    "        \n",
    "        # Update progress bar\n",
    "        if epoch % 100 == 0:\n",
    "            pbar.set_postfix({\n",
    "                'train_loss': f'{train_loss.item():.4f}',\n",
    "                'test_acc': f'{test_accuracy:.3f}'\n",
    "            })\n",
    "    \n",
    "    return {\n",
    "        'train_losses': train_losses,\n",
    "        'test_losses': test_losses,\n",
    "        'test_accuracies': test_accuracies,\n",
    "        'epochs_to_90_percent': epochs_to_90_percent,\n",
    "        'epochs_to_95_percent': epochs_to_95_percent,\n",
    "        'epochs_to_99_percent': epochs_to_99_percent,\n",
    "        'final_test_accuracy': test_accuracies[-1],\n",
    "        'final_train_loss': train_losses[-1],\n",
    "        'final_test_loss': test_losses[-1],\n",
    "        'model_state': model.state_dict(),\n",
    "        'seed': config.seed\n",
    "    }\n",
    "\n",
    "print(\"âœ“ Training function defined!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Run Multi-Seed Experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Storage for all results\n",
    "all_transfer_results = []\n",
    "all_baseline_results = []\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(f\"RUNNING {NUM_SEEDS} EXPERIMENTS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "for i, seed in enumerate(SEEDS):\n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"EXPERIMENT {i+1}/{NUM_SEEDS} - SEED {seed}\")\n",
    "    print(f\"{'='*80}\\n\")\n",
    "    \n",
    "    # Create config for this seed\n",
    "    subtraction_config = replace(\n",
    "        addition_config,\n",
    "        fn_name='subtract',\n",
    "        seed=seed\n",
    "    )\n",
    "    \n",
    "    # ========== Transfer Learning ==========\n",
    "    print(f\"\\nðŸ”„ Transfer Learning (Seed {seed})\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    transfer_model = Transformer(subtraction_config, use_cache=False)\n",
    "    transfer_model.load_state_dict(grokked_addition_model.state_dict())\n",
    "    transfer_model.to(subtraction_config.device)\n",
    "    \n",
    "    transfer_results = train_subtraction_model(\n",
    "        transfer_model,\n",
    "        subtraction_config,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        verbose=True,\n",
    "        seed_label=f\"Transfer (seed {seed})\"\n",
    "    )\n",
    "    \n",
    "    all_transfer_results.append(transfer_results)\n",
    "    \n",
    "    print(f\"âœ“ Transfer - Seed {seed}:\")\n",
    "    print(f\"  Final acc: {transfer_results['final_test_accuracy']:.4f}\")\n",
    "    print(f\"  90% at epoch: {transfer_results['epochs_to_90_percent']}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    torch.save(transfer_results, f\"{EXPERIMENT_DIR}/checkpoints/transfer_seed{seed}.pth\")\n",
    "    \n",
    "    # ========== Baseline ==========\n",
    "    print(f\"\\nðŸŽ² Baseline (Seed {seed})\")\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    baseline_model = Transformer(subtraction_config, use_cache=False)\n",
    "    baseline_model.to(subtraction_config.device)\n",
    "    \n",
    "    baseline_results = train_subtraction_model(\n",
    "        baseline_model,\n",
    "        subtraction_config,\n",
    "        num_epochs=NUM_EPOCHS,\n",
    "        verbose=True,\n",
    "        seed_label=f\"Baseline (seed {seed})\"\n",
    "    )\n",
    "    \n",
    "    all_baseline_results.append(baseline_results)\n",
    "    \n",
    "    print(f\"âœ“ Baseline - Seed {seed}:\")\n",
    "    print(f\"  Final acc: {baseline_results['final_test_accuracy']:.4f}\")\n",
    "    print(f\"  90% at epoch: {baseline_results['epochs_to_90_percent']}\")\n",
    "    \n",
    "    # Save checkpoint\n",
    "    torch.save(baseline_results, f\"{EXPERIMENT_DIR}/checkpoints/baseline_seed{seed}.pth\")\n",
    "    \n",
    "    # Free GPU memory\n",
    "    del transfer_model, baseline_model\n",
    "    torch.cuda.empty_cache()\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"ALL EXPERIMENTS COMPLETE!\")\n",
    "print(f\"{'='*80}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Aggregate Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_stats(values):\n",
    "    \"\"\"Calculate mean, std, and 95% confidence interval\"\"\"\n",
    "    values = [v for v in values if v is not None]  # Filter out None values\n",
    "    if len(values) == 0:\n",
    "        return None, None, None, None\n",
    "    \n",
    "    mean = np.mean(values)\n",
    "    std = np.std(values, ddof=1) if len(values) > 1 else 0\n",
    "    \n",
    "    # 95% confidence interval (t-distribution)\n",
    "    from scipy import stats\n",
    "    if len(values) > 1:\n",
    "        ci = stats.t.interval(0.95, len(values)-1, loc=mean, scale=stats.sem(values))\n",
    "    else:\n",
    "        ci = (mean, mean)\n",
    "    \n",
    "    return mean, std, ci[0], ci[1]\n",
    "\n",
    "# Aggregate metrics\n",
    "transfer_90_epochs = [r['epochs_to_90_percent'] for r in all_transfer_results]\n",
    "baseline_90_epochs = [r['epochs_to_90_percent'] for r in all_baseline_results]\n",
    "\n",
    "transfer_95_epochs = [r['epochs_to_95_percent'] for r in all_transfer_results]\n",
    "baseline_95_epochs = [r['epochs_to_95_percent'] for r in all_baseline_results]\n",
    "\n",
    "transfer_final_acc = [r['final_test_accuracy'] for r in all_transfer_results]\n",
    "baseline_final_acc = [r['final_test_accuracy'] for r in all_baseline_results]\n",
    "\n",
    "# Calculate statistics\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"AGGREGATED RESULTS ACROSS ALL SEEDS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nðŸ“Š Epochs to 90% Accuracy:\")\n",
    "t_mean, t_std, t_ci_low, t_ci_high = calculate_stats(transfer_90_epochs)\n",
    "b_mean, b_std, b_ci_low, b_ci_high = calculate_stats(baseline_90_epochs)\n",
    "\n",
    "print(f\"  Transfer:  {t_mean:.1f} Â± {t_std:.1f} epochs (95% CI: [{t_ci_low:.1f}, {t_ci_high:.1f}])\")\n",
    "print(f\"  Baseline:  {b_mean:.1f} Â± {b_std:.1f} epochs (95% CI: [{b_ci_low:.1f}, {b_ci_high:.1f}])\")\n",
    "\n",
    "if t_mean and b_mean:\n",
    "    speedup = b_mean / t_mean\n",
    "    improvement = b_mean - t_mean\n",
    "    print(f\"\\n  ðŸš€ Speedup: {speedup:.2f}x faster\")\n",
    "    print(f\"  ðŸ“‰ Saved: {improvement:.1f} epochs ({improvement/b_mean*100:.1f}% reduction)\")\n",
    "\n",
    "print(\"\\nðŸ“Š Epochs to 95% Accuracy:\")\n",
    "t_mean_95, t_std_95, _, _ = calculate_stats(transfer_95_epochs)\n",
    "b_mean_95, b_std_95, _, _ = calculate_stats(baseline_95_epochs)\n",
    "\n",
    "if t_mean_95:\n",
    "    print(f\"  Transfer:  {t_mean_95:.1f} Â± {t_std_95:.1f} epochs\")\n",
    "if b_mean_95:\n",
    "    print(f\"  Baseline:  {b_mean_95:.1f} Â± {b_std_95:.1f} epochs\")\n",
    "\n",
    "print(\"\\nðŸ“Š Final Test Accuracy:\")\n",
    "t_acc_mean, t_acc_std, _, _ = calculate_stats(transfer_final_acc)\n",
    "b_acc_mean, b_acc_std, _, _ = calculate_stats(baseline_final_acc)\n",
    "\n",
    "print(f\"  Transfer:  {t_acc_mean:.4f} Â± {t_acc_std:.4f}\")\n",
    "print(f\"  Baseline:  {b_acc_mean:.4f} Â± {b_acc_std:.4f}\")\n",
    "\n",
    "# Save aggregated statistics\n",
    "stats_dict = {\n",
    "    'num_seeds': NUM_SEEDS,\n",
    "    'seeds': SEEDS,\n",
    "    'transfer': {\n",
    "        'epochs_to_90': {'mean': t_mean, 'std': t_std, 'ci': [t_ci_low, t_ci_high], 'values': transfer_90_epochs},\n",
    "        'epochs_to_95': {'mean': t_mean_95, 'std': t_std_95, 'values': transfer_95_epochs},\n",
    "        'final_acc': {'mean': t_acc_mean, 'std': t_acc_std, 'values': transfer_final_acc}\n",
    "    },\n",
    "    'baseline': {\n",
    "        'epochs_to_90': {'mean': b_mean, 'std': b_std, 'ci': [b_ci_low, b_ci_high], 'values': baseline_90_epochs},\n",
    "        'epochs_to_95': {'mean': b_mean_95, 'std': b_std_95, 'values': baseline_95_epochs},\n",
    "        'final_acc': {'mean': b_acc_mean, 'std': b_acc_std, 'values': baseline_final_acc}\n",
    "    },\n",
    "    'speedup': speedup if (t_mean and b_mean) else None,\n",
    "    'improvement_epochs': improvement if (t_mean and b_mean) else None\n",
    "}\n",
    "\n",
    "# Save to JSON\n",
    "with open(f'{EXPERIMENT_DIR}/results/aggregated_stats.json', 'w') as f:\n",
    "    json.dump(stats_dict, f, indent=2)\n",
    "\n",
    "print(f\"\\nâœ“ Saved aggregated statistics to {EXPERIMENT_DIR}/results/aggregated_stats.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Visualize Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare data for plotting\n",
    "max_epochs = NUM_EPOCHS\n",
    "epochs = np.arange(max_epochs)\n",
    "\n",
    "# Stack all curves\n",
    "transfer_acc_curves = np.array([r['test_accuracies'] for r in all_transfer_results])\n",
    "baseline_acc_curves = np.array([r['test_accuracies'] for r in all_baseline_results])\n",
    "\n",
    "transfer_loss_curves = np.array([r['test_losses'] for r in all_transfer_results])\n",
    "baseline_loss_curves = np.array([r['test_losses'] for r in all_baseline_results])\n",
    "\n",
    "# Calculate mean and std\n",
    "transfer_acc_mean = transfer_acc_curves.mean(axis=0)\n",
    "transfer_acc_std = transfer_acc_curves.std(axis=0)\n",
    "\n",
    "baseline_acc_mean = baseline_acc_curves.mean(axis=0)\n",
    "baseline_acc_std = baseline_acc_curves.std(axis=0)\n",
    "\n",
    "transfer_loss_mean = transfer_loss_curves.mean(axis=0)\n",
    "transfer_loss_std = transfer_loss_curves.std(axis=0)\n",
    "\n",
    "baseline_loss_mean = baseline_loss_curves.mean(axis=0)\n",
    "baseline_loss_std = baseline_loss_curves.std(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualization\n",
    "fig, axes = plt.subplots(2, 3, figsize=(20, 12))\n",
    "\n",
    "# Plot 1: Test Accuracy (with confidence bands)\n",
    "axes[0, 0].plot(epochs, transfer_acc_mean, label='Transfer Learning', color='blue', linewidth=2)\n",
    "axes[0, 0].fill_between(epochs, \n",
    "                        transfer_acc_mean - transfer_acc_std, \n",
    "                        transfer_acc_mean + transfer_acc_std,\n",
    "                        alpha=0.3, color='blue')\n",
    "\n",
    "axes[0, 0].plot(epochs, baseline_acc_mean, label='Random Init', color='orange', linewidth=2)\n",
    "axes[0, 0].fill_between(epochs, \n",
    "                        baseline_acc_mean - baseline_acc_std, \n",
    "                        baseline_acc_mean + baseline_acc_std,\n",
    "                        alpha=0.3, color='orange')\n",
    "\n",
    "axes[0, 0].axhline(y=0.9, color='red', linestyle='--', alpha=0.5, label='90% Target')\n",
    "axes[0, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 0].set_ylabel('Test Accuracy', fontsize=12)\n",
    "axes[0, 0].set_title(f'Test Accuracy Over Time (N={NUM_SEEDS} seeds)', fontsize=14, fontweight='bold')\n",
    "axes[0, 0].legend(fontsize=10)\n",
    "axes[0, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 2: Test Loss (log scale)\n",
    "axes[0, 1].plot(epochs, np.log10(transfer_loss_mean + 1e-10), label='Transfer Learning', color='blue', linewidth=2)\n",
    "axes[0, 1].fill_between(epochs,\n",
    "                        np.log10(transfer_loss_mean - transfer_loss_std + 1e-10),\n",
    "                        np.log10(transfer_loss_mean + transfer_loss_std + 1e-10),\n",
    "                        alpha=0.3, color='blue')\n",
    "\n",
    "axes[0, 1].plot(epochs, np.log10(baseline_loss_mean + 1e-10), label='Random Init', color='orange', linewidth=2)\n",
    "axes[0, 1].fill_between(epochs,\n",
    "                        np.log10(baseline_loss_mean - baseline_loss_std + 1e-10),\n",
    "                        np.log10(baseline_loss_mean + baseline_loss_std + 1e-10),\n",
    "                        alpha=0.3, color='orange')\n",
    "\n",
    "axes[0, 1].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 1].set_ylabel('Log10(Test Loss)', fontsize=12)\n",
    "axes[0, 1].set_title(f'Test Loss Over Time (N={NUM_SEEDS} seeds)', fontsize=14, fontweight='bold')\n",
    "axes[0, 1].legend(fontsize=10)\n",
    "axes[0, 1].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Individual runs (Test Accuracy)\n",
    "for i, (seed, results) in enumerate(zip(SEEDS, all_transfer_results)):\n",
    "    axes[0, 2].plot(results['test_accuracies'], alpha=0.4, color='blue', linewidth=1)\n",
    "for i, (seed, results) in enumerate(zip(SEEDS, all_baseline_results)):\n",
    "    axes[0, 2].plot(results['test_accuracies'], alpha=0.4, color='orange', linewidth=1)\n",
    "\n",
    "axes[0, 2].plot([], [], color='blue', label='Transfer', linewidth=2)\n",
    "axes[0, 2].plot([], [], color='orange', label='Baseline', linewidth=2)\n",
    "axes[0, 2].axhline(y=0.9, color='red', linestyle='--', alpha=0.5)\n",
    "axes[0, 2].set_xlabel('Epoch', fontsize=12)\n",
    "axes[0, 2].set_ylabel('Test Accuracy', fontsize=12)\n",
    "axes[0, 2].set_title('Individual Runs (All Seeds)', fontsize=14, fontweight='bold')\n",
    "axes[0, 2].legend(fontsize=10)\n",
    "axes[0, 2].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 4: Zoomed (first 1000 epochs)\n",
    "zoom = 1000\n",
    "axes[1, 0].plot(epochs[:zoom], transfer_acc_mean[:zoom], label='Transfer', color='blue', linewidth=2)\n",
    "axes[1, 0].fill_between(epochs[:zoom],\n",
    "                        transfer_acc_mean[:zoom] - transfer_acc_std[:zoom],\n",
    "                        transfer_acc_mean[:zoom] + transfer_acc_std[:zoom],\n",
    "                        alpha=0.3, color='blue')\n",
    "\n",
    "axes[1, 0].plot(epochs[:zoom], baseline_acc_mean[:zoom], label='Baseline', color='orange', linewidth=2)\n",
    "axes[1, 0].fill_between(epochs[:zoom],\n",
    "                        baseline_acc_mean[:zoom] - baseline_acc_std[:zoom],\n",
    "                        baseline_acc_mean[:zoom] + baseline_acc_std[:zoom],\n",
    "                        alpha=0.3, color='orange')\n",
    "\n",
    "axes[1, 0].axhline(y=0.9, color='red', linestyle='--', alpha=0.5)\n",
    "\n",
    "# Mark mean 90% epochs\n",
    "if t_mean and t_mean < zoom:\n",
    "    axes[1, 0].axvline(x=t_mean, color='blue', linestyle=':', alpha=0.7, linewidth=2)\n",
    "if b_mean and b_mean < zoom:\n",
    "    axes[1, 0].axvline(x=b_mean, color='orange', linestyle=':', alpha=0.7, linewidth=2)\n",
    "\n",
    "axes[1, 0].set_xlabel('Epoch', fontsize=12)\n",
    "axes[1, 0].set_ylabel('Test Accuracy', fontsize=12)\n",
    "axes[1, 0].set_title(f'Test Accuracy (First {zoom} Epochs)', fontsize=14, fontweight='bold')\n",
    "axes[1, 0].legend(fontsize=10)\n",
    "axes[1, 0].grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 5: Box plot of epochs to 90%\n",
    "data_to_plot = [transfer_90_epochs, baseline_90_epochs]\n",
    "bp = axes[1, 1].boxplot(data_to_plot, labels=['Transfer', 'Baseline'], \n",
    "                        patch_artist=True, showmeans=True)\n",
    "bp['boxes'][0].set_facecolor('lightblue')\n",
    "bp['boxes'][1].set_facecolor('lightsalmon')\n",
    "\n",
    "axes[1, 1].set_ylabel('Epochs to 90% Accuracy', fontsize=12)\n",
    "axes[1, 1].set_title('Distribution of Epochs to 90%', fontsize=14, fontweight='bold')\n",
    "axes[1, 1].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 6: Summary statistics table\n",
    "axes[1, 2].axis('off')\n",
    "summary_text = f\"\"\"\n",
    "SUMMARY STATISTICS (N={NUM_SEEDS} seeds)\n",
    "\n",
    "Epochs to 90% Accuracy:\n",
    "  Transfer:  {t_mean:.1f} Â± {t_std:.1f}\n",
    "  Baseline:  {b_mean:.1f} Â± {b_std:.1f}\n",
    "  \n",
    "  Speedup:   {speedup:.2f}x\n",
    "  Saved:     {improvement:.1f} epochs\n",
    "  \n",
    "Final Test Accuracy:\n",
    "  Transfer:  {t_acc_mean:.4f} Â± {t_acc_std:.4f}\n",
    "  Baseline:  {b_acc_mean:.4f} Â± {b_acc_std:.4f}\n",
    "  \n",
    "Seeds: {SEEDS}\n",
    "\"\"\"\n",
    "axes[1, 2].text(0.1, 0.5, summary_text, fontsize=11, family='monospace',\n",
    "               verticalalignment='center', bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "axes[1, 2].set_title('Summary', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{EXPERIMENT_DIR}/figures/multiseed_results.png', dpi=200, bbox_inches='tight')\n",
    "print(f\"âœ“ Saved figure to {EXPERIMENT_DIR}/figures/multiseed_results.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Save All Results to Google Drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save complete results\n",
    "complete_results = {\n",
    "    'config': {\n",
    "        'num_seeds': NUM_SEEDS,\n",
    "        'seeds': SEEDS,\n",
    "        'num_epochs': NUM_EPOCHS,\n",
    "        'checkpoint_path': CHECKPOINT_PATH,\n",
    "        'timestamp': timestamp\n",
    "    },\n",
    "    'transfer_results': all_transfer_results,\n",
    "    'baseline_results': all_baseline_results,\n",
    "    'statistics': stats_dict\n",
    "}\n",
    "\n",
    "# Save as PyTorch checkpoint\n",
    "torch.save(complete_results, f'{EXPERIMENT_DIR}/results/complete_results.pth')\n",
    "print(f\"âœ“ Saved complete results to {EXPERIMENT_DIR}/results/complete_results.pth\")\n",
    "\n",
    "# Save curves as numpy for easy analysis\n",
    "np.savez(f'{EXPERIMENT_DIR}/results/curves.npz',\n",
    "         transfer_acc_curves=transfer_acc_curves,\n",
    "         baseline_acc_curves=baseline_acc_curves,\n",
    "         transfer_loss_curves=transfer_loss_curves,\n",
    "         baseline_loss_curves=baseline_loss_curves,\n",
    "         seeds=np.array(SEEDS))\n",
    "print(f\"âœ“ Saved curves to {EXPERIMENT_DIR}/results/curves.npz\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"âœ… EXPERIMENT COMPLETE - ALL RESULTS SAVED TO GOOGLE DRIVE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nResults location: {EXPERIMENT_DIR}\")\n",
    "print(\"\\nDirectory structure:\")\n",
    "print(f\"  {EXPERIMENT_DIR}/\")\n",
    "print(f\"    â”œâ”€â”€ figures/\")\n",
    "print(f\"    â”‚   â””â”€â”€ multiseed_results.png\")\n",
    "print(f\"    â”œâ”€â”€ checkpoints/\")\n",
    "print(f\"    â”‚   â”œâ”€â”€ transfer_seed*.pth ({NUM_SEEDS} files)\")\n",
    "print(f\"    â”‚   â””â”€â”€ baseline_seed*.pth ({NUM_SEEDS} files)\")\n",
    "print(f\"    â””â”€â”€ results/\")\n",
    "print(f\"        â”œâ”€â”€ complete_results.pth\")\n",
    "print(f\"        â”œâ”€â”€ curves.npz\")\n",
    "print(f\"        â””â”€â”€ aggregated_stats.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This multi-seed experiment provides statistical evidence for whether grokked modular addition models can transfer to accelerate subtraction learning.\n",
    "\n",
    "**Key Features:**\n",
    "- âœ… Multiple seeds for reproducibility\n",
    "- âœ… Statistical analysis (mean, std, confidence intervals)\n",
    "- âœ… Comprehensive visualizations\n",
    "- âœ… Persistent storage in Google Drive\n",
    "\n",
    "**Next Steps:**\n",
    "- Analyze variance across seeds\n",
    "- Test different source checkpoints\n",
    "- Try other target operations\n",
    "- Investigate which model components transfer most effectively"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
