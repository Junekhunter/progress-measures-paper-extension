{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing the Negation Hypothesis\n",
    "\n",
    "**Hypothesis**: The grokked model performs subtraction by:\n",
    "1. Negating the second input: b → (113 - b)\n",
    "2. Then performing addition: a + (113 - b) mod 113\n",
    "\n",
    "**Why this makes sense**:\n",
    "- Mathematically equivalent: (a - b) ≡ (a + (-b)) ≡ (a + (p - b)) (mod p)\n",
    "- Could reuse addition circuit (explains faster learning)\n",
    "- Explains partial frequency overlap (3/10 frequencies)\n",
    "\n",
    "**Tests we'll run**:\n",
    "1. **Embedding space test**: Check if embed(b) + embed(113-b) ≈ constant\n",
    "2. **Activation matching**: Compare activations on (a,b) for subtraction vs (a, 113-b) for addition\n",
    "3. **Intervention test**: Patch (113-b) into the model and see if it produces correct answer\n",
    "4. **Logit lens**: Check if intermediate layers show (113-b) before final output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and navigate to repo\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "if not os.path.exists('progress-measures-paper-extension'):\n",
    "    !git clone https://github.com/Junekhunter/progress-measures-paper-extension.git\n",
    "\n",
    "os.chdir('progress-measures-paper-extension')\n",
    "!pip install -q einops\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataclasses import replace\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "from transformers import Transformer, Config, gen_train_test\n",
    "import helpers\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EXPERIMENT_DIR = input(\"Enter experiment directory: \")\n",
    "SEED = 42\n",
    "p = 113\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load grokked addition model (source)\n",
    "print(\"Loading grokked addition model...\")\n",
    "addition_checkpoint = torch.load('saved_runs/wd_10-1_mod_addition_loss_curve.pth', map_location='cpu')\n",
    "\n",
    "addition_config = Config(\n",
    "    lr=1e-3,\n",
    "    weight_decay=1.0,\n",
    "    p=p,\n",
    "    d_model=128,\n",
    "    fn_name='add',\n",
    "    frac_train=0.3,\n",
    "    seed=0,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "addition_model = Transformer(addition_config, use_cache=False)\n",
    "if 'model' in addition_checkpoint:\n",
    "    addition_model.load_state_dict(addition_checkpoint['model'])\n",
    "else:\n",
    "    addition_model.load_state_dict(addition_checkpoint['state_dicts'][-1])\n",
    "addition_model.to(device)\n",
    "addition_model.eval()\n",
    "\n",
    "print(\"✓ Addition model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load grokked transfer (subtraction) model\n",
    "print(\"Loading grokked transfer (subtraction) model...\")\n",
    "\n",
    "subtraction_config = replace(addition_config, fn_name='subtract', seed=SEED)\n",
    "checkpoint_path = f'{EXPERIMENT_DIR}/checkpoints/grokked_transfer_seed{SEED}.pth'\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "subtraction_model = Transformer(subtraction_config, use_cache=False)\n",
    "subtraction_model.load_state_dict(checkpoint['model_state'], strict=True)\n",
    "subtraction_model.to(device)\n",
    "subtraction_model.eval()\n",
    "\n",
    "print(f\"✓ Subtraction model loaded\")\n",
    "print(f\"  Accuracy: {checkpoint['final_test_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 1: Embedding Space Analysis\n",
    "\n",
    "**Hypothesis**: If the model negates `b`, then `embed(b) + embed(113-b)` should be approximately constant.\n",
    "\n",
    "This would indicate a learned negation operation in embedding space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"TEST 1: Embedding Space Analysis\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get embedding matrix from subtraction model\n",
    "W_E = subtraction_model.embed.W_E.data  # [d_model, d_vocab]\n",
    "\n",
    "# For each b, compute embed(b) + embed(113-b)\n",
    "negation_sums = []\n",
    "\n",
    "for b in range(p):\n",
    "    neg_b = (p - b) % p\n",
    "    \n",
    "    embed_b = W_E[:, b]  # [d_model]\n",
    "    embed_neg_b = W_E[:, neg_b]  # [d_model]\n",
    "    \n",
    "    sum_vec = embed_b + embed_neg_b\n",
    "    negation_sums.append(sum_vec.cpu().numpy())\n",
    "\n",
    "negation_sums = np.array(negation_sums)  # [p, d_model]\n",
    "\n",
    "# Check if all sums are similar (low variance)\n",
    "mean_sum = negation_sums.mean(axis=0)  # [d_model]\n",
    "variance_across_b = negation_sums.var(axis=0).mean()  # scalar\n",
    "\n",
    "print(f\"\\nVariance of embed(b) + embed(113-b) across all b:\")\n",
    "print(f\"  Mean variance per dimension: {variance_across_b:.6f}\")\n",
    "\n",
    "# Compare to baseline: variance of individual embeddings\n",
    "baseline_variance = W_E.T.cpu().numpy().var(axis=0).mean()\n",
    "print(f\"  Baseline (individual embed variance): {baseline_variance:.6f}\")\n",
    "\n",
    "reduction_ratio = variance_across_b / baseline_variance\n",
    "print(f\"\\n  Variance reduction: {reduction_ratio:.4f}\")\n",
    "\n",
    "if reduction_ratio < 0.1:\n",
    "    print(f\"\\n✓ STRONG EVIDENCE for negation in embedding space!\")\n",
    "    print(f\"  embed(b) + embed(113-b) is nearly constant\")\n",
    "    print(f\"  This suggests the model learned: b → (113-b) transformation\")\n",
    "elif reduction_ratio < 0.5:\n",
    "    print(f\"\\n→ MODERATE EVIDENCE for negation\")\n",
    "    print(f\"  Some structure, but not perfectly constant\")\n",
    "else:\n",
    "    print(f\"\\n✗ NO EVIDENCE for negation in embedding space\")\n",
    "    print(f\"  embed(b) + embed(113-b) varies as much as individual embeddings\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize embedding negation pairs\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Plot 1: Heatmap of embed(b) + embed(113-b) for all b\n",
    "ax = axes[0]\n",
    "im = ax.imshow(negation_sums.T, aspect='auto', cmap='RdBu_r', \n",
    "               vmin=negation_sums.min(), vmax=negation_sums.max())\n",
    "ax.set_xlabel('Token b', fontsize=12)\n",
    "ax.set_ylabel('Embedding Dimension', fontsize=12)\n",
    "ax.set_title('embed(b) + embed(113-b) for all b', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Plot 2: Variance per dimension\n",
    "ax = axes[1]\n",
    "variance_per_dim = negation_sums.var(axis=0)\n",
    "ax.bar(range(len(variance_per_dim)), variance_per_dim, alpha=0.7, edgecolor='black')\n",
    "ax.axhline(baseline_variance, color='red', linestyle='--', \n",
    "           label=f'Baseline: {baseline_variance:.4f}', linewidth=2)\n",
    "ax.set_xlabel('Embedding Dimension', fontsize=12)\n",
    "ax.set_ylabel('Variance across b', fontsize=12)\n",
    "ax.set_title('Variance of Negation Sums per Dimension', fontsize=14, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{EXPERIMENT_DIR}/figures/embedding_negation_test.png', dpi=200, bbox_inches='tight')\n",
    "print(\"✓ Saved: embedding_negation_test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 2: Activation Matching\n",
    "\n",
    "**Test**: Compare activations when:\n",
    "- Subtraction model sees (a, b) for subtraction\n",
    "- Addition model sees (a, 113-b) for addition\n",
    "\n",
    "If hypothesis is true, activations should be very similar after the embedding layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 2: Activation Matching\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Choose test examples\n",
    "test_examples = [\n",
    "    (50, 30),   # 50 - 30 = 20\n",
    "    (100, 75),  # 100 - 75 = 25\n",
    "    (20, 80),   # 20 - 80 = -60 ≡ 53 (mod 113)\n",
    "    (0, 50),    # 0 - 50 = -50 ≡ 63 (mod 113)\n",
    "]\n",
    "\n",
    "activation_comparisons = []\n",
    "\n",
    "for a, b in test_examples:\n",
    "    # Ground truth\n",
    "    subtraction_answer = (a - b) % p\n",
    "    \n",
    "    # Negation hypothesis prediction\n",
    "    neg_b = (p - b) % p\n",
    "    addition_answer_with_negation = (a + neg_b) % p\n",
    "    \n",
    "    print(f\"\\nExample: ({a}, {b})\")\n",
    "    print(f\"  Subtraction (a - b):     {subtraction_answer}\")\n",
    "    print(f\"  Addition (a + (113-b)):  {addition_answer_with_negation}\")\n",
    "    print(f\"  Match: {subtraction_answer == addition_answer_with_negation}\")\n",
    "    \n",
    "    # Get activations from subtraction model on (a, b)\n",
    "    sub_input = torch.tensor([[a, b, p]]).to(device)\n",
    "    \n",
    "    cache_sub = {}\n",
    "    subtraction_model.remove_all_hooks()\n",
    "    subtraction_model.cache_all(cache_sub)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        sub_logits = subtraction_model(sub_input)\n",
    "        sub_pred = sub_logits[0, -1, :p].argmax().item()\n",
    "    \n",
    "    # Get activations from addition model on (a, 113-b)\n",
    "    add_input = torch.tensor([[a, neg_b, p]]).to(device)\n",
    "    \n",
    "    cache_add = {}\n",
    "    addition_model.remove_all_hooks()\n",
    "    addition_model.cache_all(cache_add)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        add_logits = addition_model(add_input)\n",
    "        add_pred = add_logits[0, -1, :p].argmax().item()\n",
    "    \n",
    "    print(f\"  Subtraction model predicts: {sub_pred} (correct: {sub_pred == subtraction_answer})\")\n",
    "    print(f\"  Addition model predicts:    {add_pred} (correct: {add_pred == addition_answer_with_negation})\")\n",
    "    \n",
    "    # Compare MLP activations (after position 1, which is 'b')\n",
    "    sub_mlp_acts = cache_sub['blocks.0.mlp.hook_post'][0, 1, :].cpu().numpy()  # [d_mlp]\n",
    "    add_mlp_acts = cache_add['blocks.0.mlp.hook_post'][0, 1, :].cpu().numpy()  # [d_mlp]\n",
    "    \n",
    "    # Compute correlation\n",
    "    correlation = np.corrcoef(sub_mlp_acts, add_mlp_acts)[0, 1]\n",
    "    \n",
    "    # Compute cosine similarity\n",
    "    from scipy.spatial.distance import cosine\n",
    "    cos_sim = 1 - cosine(sub_mlp_acts, add_mlp_acts)\n",
    "    \n",
    "    print(f\"  MLP activation correlation: {correlation:.3f}\")\n",
    "    print(f\"  MLP activation cosine similarity: {cos_sim:.3f}\")\n",
    "    \n",
    "    activation_comparisons.append({\n",
    "        'a': a,\n",
    "        'b': b,\n",
    "        'correlation': correlation,\n",
    "        'cos_sim': cos_sim\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "mean_corr = np.mean([x['correlation'] for x in activation_comparisons])\n",
    "mean_cos = np.mean([x['cos_sim'] for x in activation_comparisons])\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUMMARY:\")\n",
    "print(f\"  Mean correlation: {mean_corr:.3f}\")\n",
    "print(f\"  Mean cosine similarity: {mean_cos:.3f}\")\n",
    "\n",
    "if mean_corr > 0.8:\n",
    "    print(f\"\\n✓ STRONG EVIDENCE: Activations are very similar!\")\n",
    "    print(f\"  Subtraction model likely computes (a + (113-b))\")\n",
    "elif mean_corr > 0.5:\n",
    "    print(f\"\\n→ MODERATE EVIDENCE: Some similarity in activations\")\n",
    "else:\n",
    "    print(f\"\\n✗ WEAK EVIDENCE: Activations are quite different\")\n",
    "    print(f\"  Models use different computational strategies\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 3: Intervention Test\n",
    "\n",
    "**Direct test**: Patch the subtraction model's input embeddings.\n",
    "\n",
    "If we replace embed(b) with embed(113-b), does the model still produce the correct subtraction answer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 3: Embedding Intervention\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def intervene_embedding(model, input_tensor, position, new_value):\n",
    "    \"\"\"\n",
    "    Run model but replace embedding at `position` with embedding of `new_value`.\n",
    "    \"\"\"\n",
    "    # Hook to replace embedding\n",
    "    def replace_hook(tensor, hook):\n",
    "        # tensor shape: [batch, seq, d_model]\n",
    "        new_embed = model.embed.W_E[:, new_value]  # [d_model]\n",
    "        tensor[:, position, :] = new_embed\n",
    "        return tensor\n",
    "    \n",
    "    # Add hook\n",
    "    model.remove_all_hooks()\n",
    "    hook_point = None\n",
    "    for name, module in model.named_modules():\n",
    "        if 'embed' in name and hasattr(module, 'add_hook'):\n",
    "            # We want to hook after embedding\n",
    "            pass\n",
    "    \n",
    "    # Actually, let's do this more directly\n",
    "    with torch.no_grad():\n",
    "        # Get embeddings\n",
    "        x = model.embed(input_tensor)  # [batch, seq, d_model]\n",
    "        \n",
    "        # Replace embedding at position with new value\n",
    "        x[:, position, :] = model.embed.W_E[:, new_value]\n",
    "        \n",
    "        # Continue through rest of model\n",
    "        x = model.pos_embed(x)\n",
    "        for block in model.blocks:\n",
    "            x = block(x)\n",
    "        logits = model.unembed(x)\n",
    "        \n",
    "    return logits\n",
    "\n",
    "# Test on examples\n",
    "print(\"\\nIntervention: Replace embed(b) with embed(113-b)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "intervention_results = []\n",
    "\n",
    "for a, b in test_examples:\n",
    "    subtraction_answer = (a - b) % p\n",
    "    neg_b = (p - b) % p\n",
    "    \n",
    "    # Normal subtraction prediction\n",
    "    normal_input = torch.tensor([[a, b, p]]).to(device)\n",
    "    with torch.no_grad():\n",
    "        normal_logits = subtraction_model(normal_input)\n",
    "        normal_pred = normal_logits[0, -1, :p].argmax().item()\n",
    "    \n",
    "    # Intervened prediction (replace b with 113-b)\n",
    "    intervened_logits = intervene_embedding(subtraction_model, normal_input, position=1, new_value=neg_b)\n",
    "    intervened_pred = intervened_logits[0, -1, :p].argmax().item()\n",
    "    \n",
    "    print(f\"\\n({a}, {b}): True answer = {subtraction_answer}\")\n",
    "    print(f\"  Normal:      {normal_pred} {'✓' if normal_pred == subtraction_answer else '✗'}\")\n",
    "    print(f\"  Intervened:  {intervened_pred} {'✓' if intervened_pred == subtraction_answer else '✗'}\")\n",
    "    \n",
    "    # Check if intervention broke it or kept it working\n",
    "    normal_correct = (normal_pred == subtraction_answer)\n",
    "    intervened_correct = (intervened_pred == subtraction_answer)\n",
    "    \n",
    "    intervention_results.append({\n",
    "        'a': a,\n",
    "        'b': b,\n",
    "        'normal_correct': normal_correct,\n",
    "        'intervened_correct': intervened_correct\n",
    "    })\n",
    "\n",
    "# Summary\n",
    "normal_accuracy = np.mean([x['normal_correct'] for x in intervention_results])\n",
    "intervened_accuracy = np.mean([x['intervened_correct'] for x in intervention_results])\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"SUMMARY:\")\n",
    "print(f\"  Normal accuracy:     {normal_accuracy*100:.0f}%\")\n",
    "print(f\"  Intervened accuracy: {intervened_accuracy*100:.0f}%\")\n",
    "\n",
    "if intervened_accuracy > 0.8:\n",
    "    print(f\"\\n✓ STRONG SUPPORT for negation hypothesis!\")\n",
    "    print(f\"  Replacing b → (113-b) PRESERVES correctness\")\n",
    "    print(f\"  Model is doing: a + (113-b) internally\")\n",
    "elif intervened_accuracy < 0.2:\n",
    "    print(f\"\\n✗ REFUTES negation hypothesis\")\n",
    "    print(f\"  Replacing b → (113-b) BREAKS the model\")\n",
    "    print(f\"  Model is NOT doing negation + addition\")\n",
    "else:\n",
    "    print(f\"\\n→ INCONCLUSIVE\")\n",
    "    print(f\"  Mixed results - may use negation for some cases\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test 4: Check Predictions on Systematic Grid\n",
    "\n",
    "Test if predictions match the negation hypothesis across many examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"TEST 4: Systematic Grid Test\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Test on all possible inputs\n",
    "all_inputs = torch.tensor([(i, j, p) for i in range(p) for j in range(p)]).to(device)\n",
    "\n",
    "# Get subtraction model predictions\n",
    "with torch.no_grad():\n",
    "    sub_logits = subtraction_model(all_inputs)\n",
    "    sub_preds = sub_logits[:, -1, :p].argmax(dim=-1).cpu().numpy()\n",
    "\n",
    "# Expected answers\n",
    "true_answers = np.array([(i - j) % p for i in range(p) for j in range(p)])\n",
    "\n",
    "# Check if model is correct\n",
    "correct = (sub_preds == true_answers)\n",
    "accuracy = correct.mean()\n",
    "\n",
    "print(f\"\\nOverall accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "# Now check: for incorrect predictions, do they match (a + b) instead of (a - b)?\n",
    "incorrect_mask = ~correct\n",
    "incorrect_indices = np.where(incorrect_mask)[0]\n",
    "\n",
    "if len(incorrect_indices) > 0:\n",
    "    print(f\"\\nAnalyzing {len(incorrect_indices)} incorrect predictions...\")\n",
    "    \n",
    "    # Check if errors match addition\n",
    "    addition_answers = np.array([(i + j) % p for i in range(p) for j in range(p)])\n",
    "    errors_match_addition = (sub_preds[incorrect_mask] == addition_answers[incorrect_mask]).mean()\n",
    "    \n",
    "    print(f\"  Errors that match ADDITION (a+b): {errors_match_addition*100:.1f}%\")\n",
    "    \n",
    "    # Check if errors match double subtraction\n",
    "    double_sub = np.array([(i - 2*j) % p for i in range(p) for j in range(p)])\n",
    "    errors_match_double = (sub_preds[incorrect_mask] == double_sub[incorrect_mask]).mean()\n",
    "    \n",
    "    print(f\"  Errors that match (a-2b):        {errors_match_double*100:.1f}%\")\n",
    "    \n",
    "    if errors_match_addition > 0.5:\n",
    "        print(f\"\\n  → Many errors are doing ADDITION instead of SUBTRACTION\")\n",
    "        print(f\"    This suggests incomplete learning, not negation hypothesis\")\n",
    "else:\n",
    "    print(f\"\\n✓ Perfect accuracy - no errors to analyze\")\n",
    "\n",
    "# Visualize error pattern\n",
    "if len(incorrect_indices) > 0:\n",
    "    error_matrix = correct.reshape(p, p).astype(float)\n",
    "    \n",
    "    plt.figure(figsize=(10, 8))\n",
    "    plt.imshow(error_matrix, cmap='RdYlGn', vmin=0, vmax=1, aspect='auto')\n",
    "    plt.colorbar(label='Correct (1) vs Incorrect (0)')\n",
    "    plt.xlabel('b', fontsize=12)\n",
    "    plt.ylabel('a', fontsize=12)\n",
    "    plt.title('Correctness Matrix: Subtraction (a - b) mod 113', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f'{EXPERIMENT_DIR}/figures/subtraction_correctness_matrix.png', dpi=200, bbox_inches='tight')\n",
    "    print(\"\\n✓ Saved: subtraction_correctness_matrix.png\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"NEGATION HYPOTHESIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\nHypothesis: Model does subtraction via (a + (113 - b)) mod 113\")\n",
    "print(\"\\nEvidence:\")\n",
    "print(f\"  1. Embedding negation structure: {reduction_ratio:.4f} variance reduction\")\n",
    "print(f\"  2. Activation matching: {mean_corr:.3f} correlation\")\n",
    "print(f\"  3. Intervention test: {intervened_accuracy*100:.0f}% accuracy after replacing b\")\n",
    "print(f\"  4. Overall accuracy: {accuracy*100:.2f}%\")\n",
    "\n",
    "print(\"\\nConclusion:\")\n",
    "\n",
    "# Decision logic\n",
    "strong_evidence = 0\n",
    "if reduction_ratio < 0.1:\n",
    "    strong_evidence += 1\n",
    "if mean_corr > 0.8:\n",
    "    strong_evidence += 1\n",
    "if intervened_accuracy > 0.8:\n",
    "    strong_evidence += 1\n",
    "\n",
    "if strong_evidence >= 2:\n",
    "    print(\"\\n✓ HYPOTHESIS SUPPORTED\")\n",
    "    print(\"  The grokked model appears to compute subtraction by:\")\n",
    "    print(\"  1. Negating the second input: b → (113 - b)\")\n",
    "    print(\"  2. Reusing addition circuit: a + (113 - b)\")\n",
    "    print(\"\\n  This explains:\")\n",
    "    print(\"  - Faster convergence (reusing learned addition)\")\n",
    "    print(\"  - Partial frequency overlap (shared addition core)\")\n",
    "    print(\"  - Lower specialization (distributed negation + addition)\")\n",
    "elif strong_evidence == 1:\n",
    "    print(\"\\n→ HYPOTHESIS PARTIALLY SUPPORTED\")\n",
    "    print(\"  Some evidence for negation mechanism, but not conclusive\")\n",
    "    print(\"  Model may use negation for some cases but not all\")\n",
    "else:\n",
    "    print(\"\\n✗ HYPOTHESIS NOT SUPPORTED\")\n",
    "    print(\"  Model does NOT appear to use simple negation + addition\")\n",
    "    print(\"  It likely learned a distinct subtraction algorithm\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
