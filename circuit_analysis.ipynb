{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Circuit Analysis: Mechanistic Interpretability of Transfer Learning\n",
    "\n",
    "**Goal**: Use the interpretability tools from the original paper to examine:\n",
    "1. What circuit the grokked addition model learned\n",
    "2. What circuits the transferred models learned (grokked, memorized, random)\n",
    "3. How the circuits differ between conditions\n",
    "4. Whether the grokked algorithm actually transfers\n",
    "\n",
    "**Key Analyses**:\n",
    "- Fourier analysis of logits\n",
    "- Neuron frequency specialization\n",
    "- Attention pattern visualization\n",
    "- Circuit component attribution\n",
    "- Progressive development during training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and navigate to repo\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "if not os.path.exists('progress-measures-paper-extension'):\n",
    "    !git clone https://github.com/Junekhunter/progress-measures-paper-extension.git\n",
    "\n",
    "os.chdir('progress-measures-paper-extension')\n",
    "!pip install -q einops\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "from dataclasses import replace\n",
    "\n",
    "from transformers import (\n",
    "    Transformer, Config, gen_train_test,\n",
    "    make_fourier_basis, calculate_key_freqs,\n",
    "    calculate_trig_loss, calculate_coefficients,\n",
    "    calculate_excluded_loss\n",
    ")\n",
    "import helpers\n",
    "\n",
    "print(\"✓ Imports successful\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EXPERIMENT_DIR = input(\"Enter your experiment directory path (e.g., /content/drive/MyDrive/grokking_transfer_experiments/3way_run_YYYYMMDD_HHMMSS): \")\n",
    "SEED_TO_ANALYZE = 42  # Use a typical seed (not the outlier)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Analyzing seed: {SEED_TO_ANALYZE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 1: Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the source grokked addition model\n",
    "print(\"Loading source grokked addition model...\")\n",
    "addition_checkpoint = torch.load('saved_runs/wd_10-1_mod_addition_loss_curve.pth', map_location='cpu')\n",
    "\n",
    "addition_config = Config(\n",
    "    lr=1e-3,\n",
    "    weight_decay=1.0,\n",
    "    p=113,\n",
    "    d_model=128,\n",
    "    fn_name='add',\n",
    "    frac_train=0.3,\n",
    "    seed=0,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "grokked_addition_model = Transformer(addition_config, use_cache=False)\n",
    "if 'model' in addition_checkpoint:\n",
    "    grokked_addition_model.load_state_dict(addition_checkpoint['model'])\n",
    "else:\n",
    "    grokked_addition_model.load_state_dict(addition_checkpoint['state_dicts'][-1])\n",
    "grokked_addition_model.to(device)\n",
    "grokked_addition_model.eval()\n",
    "\n",
    "print(\"✓ Grokked addition model loaded\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the three subtraction models for comparison\n",
    "subtraction_config = replace(addition_config, fn_name='subtract', seed=SEED_TO_ANALYZE)\n",
    "\n",
    "models = {}\n",
    "\n",
    "for condition in ['grokked_transfer', 'memorized_transfer', 'random_baseline']:\n",
    "    print(f\"Loading {condition} model (seed {SEED_TO_ANALYZE})...\")\n",
    "    \n",
    "    checkpoint_path = f'{EXPERIMENT_DIR}/checkpoints/{condition}_seed{SEED_TO_ANALYZE}.pth'\n",
    "    checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "    \n",
    "    model = Transformer(subtraction_config, use_cache=False)\n",
    "    model.load_state_dict(checkpoint['model_state'])\n",
    "    model.to(device)\n",
    "    model.eval()\n",
    "    \n",
    "    models[condition] = {\n",
    "        'model': model,\n",
    "        'checkpoint': checkpoint,\n",
    "        'final_accuracy': checkpoint['final_test_accuracy'],\n",
    "        'epochs_to_999': checkpoint['threshold_epochs'].get(0.999, None)\n",
    "    }\n",
    "    \n",
    "    print(f\"  Final accuracy: {checkpoint['final_test_accuracy']:.4f}\")\n",
    "    print(f\"  Epochs to 99.9%: {checkpoint['threshold_epochs'].get(0.999, 'Not reached')}\")\n",
    "\n",
    "print(\"\\n✓ All models loaded\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2: Fourier Analysis of Logits\n",
    "\n",
    "For modular arithmetic, the model can represent the answer using Fourier components.  \n",
    "We analyze which frequencies the model uses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate all possible inputs\n",
    "p = addition_config.p\n",
    "all_data_add = torch.tensor([(i, j, p) for i in range(p) for j in range(p)]).to(device)\n",
    "all_data_sub = torch.tensor([(i, j, p) for i in range(p) for j in range(p)]).to(device)\n",
    "\n",
    "# Create Fourier basis\n",
    "fourier_basis = make_fourier_basis(addition_config)\n",
    "\n",
    "print(f\"Analyzing {p*p} input pairs\")\n",
    "print(f\"Fourier basis size: {fourier_basis.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze grokked addition model\n",
    "print(\"Analyzing GROKKED ADDITION model...\")\n",
    "\n",
    "with torch.no_grad():\n",
    "    logits_add = grokked_addition_model(all_data_add)[:, -1, :p]\n",
    "\n",
    "# Calculate Fourier coefficients\n",
    "key_freqs_add = calculate_key_freqs(addition_config, grokked_addition_model, all_data_add)\n",
    "coeffs_add = calculate_coefficients(logits_add, fourier_basis, key_freqs_add, p, device)\n",
    "\n",
    "print(f\"Key frequencies for addition: {key_freqs_add}\")\n",
    "print(f\"Coefficient magnitudes: {[f'{c.abs().mean():.3f}' for c in coeffs_add]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze all three subtraction models\n",
    "print(\"\\nAnalyzing SUBTRACTION models...\\n\")\n",
    "\n",
    "fourier_analysis = {}\n",
    "\n",
    "for condition, data in models.items():\n",
    "    print(f\"Analyzing {condition}...\")\n",
    "    model = data['model']\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        logits = model(all_data_sub)[:, -1, :p]\n",
    "    \n",
    "    # Key frequencies\n",
    "    key_freqs = calculate_key_freqs(subtraction_config, model, all_data_sub)\n",
    "    \n",
    "    # Fourier coefficients\n",
    "    coeffs = calculate_coefficients(logits, fourier_basis, key_freqs, p, device)\n",
    "    \n",
    "    fourier_analysis[condition] = {\n",
    "        'logits': logits,\n",
    "        'key_freqs': key_freqs,\n",
    "        'coefficients': coeffs\n",
    "    }\n",
    "    \n",
    "    print(f\"  Key frequencies: {key_freqs}\")\n",
    "    print(f\"  Coefficient magnitudes: {[f'{c.abs().mean():.3f}' for c in coeffs]}\")\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Visualize Fourier coefficients comparison\nfig, axes = plt.subplots(1, 4, figsize=(20, 4))\n\n# Helper function to compute coefficient magnitude (handles different shapes)\ndef get_coeff_magnitude(coeff):\n    \"\"\"Compute mean absolute value, handling various tensor shapes.\"\"\"\n    abs_coeff = coeff.abs()\n    # Reduce to scalar by taking mean over all dimensions\n    while abs_coeff.dim() > 0:\n        abs_coeff = abs_coeff.mean()\n    return abs_coeff.item()\n\n# Plot for addition\nax = axes[0]\ncoeff_magnitudes_add = np.array([get_coeff_magnitude(c) for c in coeffs_add])\nax.bar(range(len(key_freqs_add)), coeff_magnitudes_add, color='green', alpha=0.7)\nax.set_xlabel('Frequency')\nax.set_ylabel('Mean |Coefficient|')\nax.set_title('Grokked Addition\\n(Source Model)', fontweight='bold')\nax.set_xticks(range(len(key_freqs_add)))\nax.set_xticklabels(key_freqs_add)\nax.grid(True, alpha=0.3, axis='y')\n\n# Plot for each subtraction condition\ncolors = {'grokked_transfer': 'blue', 'memorized_transfer': 'purple', 'random_baseline': 'orange'}\ntitles = {'grokked_transfer': 'Grokked Transfer', 'memorized_transfer': 'Memorized Transfer', 'random_baseline': 'Random Baseline'}\n\nfor idx, condition in enumerate(['grokked_transfer', 'memorized_transfer', 'random_baseline']):\n    ax = axes[idx + 1]\n    \n    coeffs = fourier_analysis[condition]['coefficients']\n    key_freqs = fourier_analysis[condition]['key_freqs']\n    \n    coeff_mags = np.array([get_coeff_magnitude(c) for c in coeffs])\n    \n    ax.bar(range(len(key_freqs)), coeff_mags, color=colors[condition], alpha=0.7)\n    ax.set_xlabel('Frequency')\n    ax.set_ylabel('Mean |Coefficient|')\n    ax.set_title(f'{titles[condition]}\\n(Subtraction)', fontweight='bold')\n    ax.set_xticks(range(len(key_freqs)))\n    ax.set_xticklabels(key_freqs)\n    ax.grid(True, alpha=0.3, axis='y')\n\nplt.tight_layout()\nplt.savefig(f'{EXPERIMENT_DIR}/figures/fourier_coefficients_comparison.png', dpi=200, bbox_inches='tight')\nprint(\"✓ Saved: fourier_coefficients_comparison.png\")\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 3: Neuron Frequency Specialization\n",
    "\n",
    "Analyze which frequencies each MLP neuron responds to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_neuron_frequencies(model, config, all_data, model_name):\n",
    "    \"\"\"\n",
    "    Analyze which frequency each neuron specializes in.\n",
    "    Returns array of shape (d_mlp,) with the dominant frequency for each neuron.\n",
    "    \"\"\"\n",
    "    labels = torch.tensor([config.fn(i, j) for i, j, _ in all_data]).to(config.device)\n",
    "    \n",
    "    cache = {}\n",
    "    model.remove_all_hooks()\n",
    "    model.cache_all(cache)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model(all_data)\n",
    "    \n",
    "    neuron_acts = cache['blocks.0.mlp.hook_post'][:, -1]\n",
    "    \n",
    "    # Center neurons\n",
    "    import einops\n",
    "    neuron_acts_centered = neuron_acts - einops.reduce(neuron_acts, 'batch neuron -> 1 neuron', 'mean')\n",
    "    \n",
    "    # Fourier transform\n",
    "    fourier_basis = make_fourier_basis(config)\n",
    "    fourier_neuron_acts = helpers.fft2d(neuron_acts_centered, p=config.p, fourier_basis=fourier_basis)\n",
    "    fourier_neuron_acts_square = fourier_neuron_acts.reshape(config.p, config.p, config.d_mlp)\n",
    "    \n",
    "    # Find dominant frequency for each neuron\n",
    "    neuron_freqs = []\n",
    "    neuron_frac_explained = []\n",
    "    \n",
    "    for ni in range(config.d_mlp):\n",
    "        best_frac = -1e6\n",
    "        best_freq = -1\n",
    "        \n",
    "        for freq in range(1, config.p//2):\n",
    "            numerator = helpers.extract_freq_2d(fourier_neuron_acts_square[:, :, ni], freq, p=config.p).pow(2).sum()\n",
    "            denominator = fourier_neuron_acts_square[:, :, ni].pow(2).sum().item()\n",
    "            frac_explained = numerator / (denominator + 1e-10)\n",
    "            \n",
    "            if frac_explained > best_frac:\n",
    "                best_freq = freq\n",
    "                best_frac = frac_explained\n",
    "        \n",
    "        neuron_freqs.append(best_freq)\n",
    "        neuron_frac_explained.append(best_frac.item())\n",
    "    \n",
    "    neuron_freqs = np.array(neuron_freqs)\n",
    "    neuron_frac_explained = np.array(neuron_frac_explained)\n",
    "    \n",
    "    print(f\"\\n{model_name}:\")\n",
    "    print(f\"  Unique frequencies used: {np.unique(neuron_freqs)}\")\n",
    "    print(f\"  Mean fraction explained: {neuron_frac_explained.mean():.3f}\")\n",
    "    print(f\"  Neurons with >50% variance explained: {(neuron_frac_explained > 0.5).sum()}/{config.d_mlp}\")\n",
    "    \n",
    "    return neuron_freqs, neuron_frac_explained\n",
    "\n",
    "# Analyze all models\n",
    "print(\"Analyzing neuron frequency specialization...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "neuron_analysis = {}\n",
    "\n",
    "# Addition model\n",
    "freqs_add, frac_add = analyze_neuron_frequencies(\n",
    "    grokked_addition_model, addition_config, all_data_add, \"Grokked Addition\"\n",
    ")\n",
    "neuron_analysis['addition'] = {'freqs': freqs_add, 'frac_explained': frac_add}\n",
    "\n",
    "# Subtraction models\n",
    "for condition in ['grokked_transfer', 'memorized_transfer', 'random_baseline']:\n",
    "    model = models[condition]['model']\n",
    "    freqs, frac = analyze_neuron_frequencies(\n",
    "        model, subtraction_config, all_data_sub, condition.replace('_', ' ').title()\n",
    "    )\n",
    "    neuron_analysis[condition] = {'freqs': freqs, 'frac_explained': frac}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize neuron frequency specialization\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "\n",
    "models_to_plot = [\n",
    "    ('addition', 'Grokked Addition (Source)', 'green'),\n",
    "    ('grokked_transfer', 'Grokked Transfer', 'blue'),\n",
    "    ('memorized_transfer', 'Memorized Transfer', 'purple'),\n",
    "    ('random_baseline', 'Random Baseline', 'orange')\n",
    "]\n",
    "\n",
    "for idx, (key, title, color) in enumerate(models_to_plot):\n",
    "    row = idx // 2\n",
    "    col = idx % 2\n",
    "    ax = axes[row, col]\n",
    "    \n",
    "    freqs = neuron_analysis[key]['freqs']\n",
    "    frac = neuron_analysis[key]['frac_explained']\n",
    "    \n",
    "    # Histogram of neuron frequencies\n",
    "    unique_freqs, counts = np.unique(freqs, return_counts=True)\n",
    "    \n",
    "    bars = ax.bar(unique_freqs, counts, color=color, alpha=0.7, edgecolor='black')\n",
    "    \n",
    "    # Color bars by mean fraction explained for that frequency\n",
    "    for i, freq in enumerate(unique_freqs):\n",
    "        mask = freqs == freq\n",
    "        mean_frac = frac[mask].mean()\n",
    "        bars[i].set_alpha(0.3 + 0.6 * mean_frac)  # Darker = better explained\n",
    "    \n",
    "    ax.set_xlabel('Frequency', fontsize=12)\n",
    "    ax.set_ylabel('Number of Neurons', fontsize=12)\n",
    "    ax.set_title(title, fontsize=14, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Add text with mean fraction explained\n",
    "    ax.text(0.98, 0.98, f'Mean explained: {frac.mean():.2f}',\n",
    "           transform=ax.transAxes, ha='right', va='top',\n",
    "           bbox=dict(boxstyle='round', facecolor='white', alpha=0.8),\n",
    "           fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{EXPERIMENT_DIR}/figures/neuron_frequency_specialization.png', dpi=200, bbox_inches='tight')\n",
    "print(\"✓ Saved: neuron_frequency_specialization.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 4: Attention Pattern Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_attention_patterns(model, data_sample, num_examples=50):\n",
    "    \"\"\"\n",
    "    Extract attention patterns from the model.\n",
    "    Returns mean attention pattern across examples.\n",
    "    \"\"\"\n",
    "    cache = {}\n",
    "    model.remove_all_hooks()\n",
    "    model.cache_all(cache)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        model(data_sample[:num_examples])\n",
    "    \n",
    "    # Get attention patterns (batch, heads, query_pos, key_pos)\n",
    "    attn = cache['blocks.0.attn.hook_attn']\n",
    "    \n",
    "    # Average over batch\n",
    "    attn_mean = attn.mean(dim=0)  # (heads, query_pos, key_pos)\n",
    "    \n",
    "    return attn_mean.cpu().numpy()\n",
    "\n",
    "# Get attention patterns for all models\n",
    "print(\"Extracting attention patterns...\")\n",
    "\n",
    "attn_patterns = {}\n",
    "\n",
    "# Addition\n",
    "attn_patterns['addition'] = get_attention_patterns(grokked_addition_model, all_data_add)\n",
    "\n",
    "# Subtraction models\n",
    "for condition in ['grokked_transfer', 'memorized_transfer', 'random_baseline']:\n",
    "    attn_patterns[condition] = get_attention_patterns(models[condition]['model'], all_data_sub)\n",
    "\n",
    "print(f\"✓ Attention patterns extracted (shape: {attn_patterns['addition'].shape})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize attention patterns\n",
    "num_heads = attn_patterns['addition'].shape[0]\n",
    "\n",
    "fig, axes = plt.subplots(4, num_heads, figsize=(4*num_heads, 16))\n",
    "\n",
    "models_to_plot = [\n",
    "    ('addition', 'Grokked Addition'),\n",
    "    ('grokked_transfer', 'Grokked Transfer'),\n",
    "    ('memorized_transfer', 'Memorized Transfer'),\n",
    "    ('random_baseline', 'Random Baseline')\n",
    "]\n",
    "\n",
    "for row, (key, title) in enumerate(models_to_plot):\n",
    "    attn = attn_patterns[key]\n",
    "    \n",
    "    for head in range(num_heads):\n",
    "        ax = axes[row, head]\n",
    "        \n",
    "        im = ax.imshow(attn[head], cmap='viridis', aspect='auto')\n",
    "        \n",
    "        if row == 0:\n",
    "            ax.set_title(f'Head {head}', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        if head == 0:\n",
    "            ax.set_ylabel(title, fontsize=12, fontweight='bold')\n",
    "        \n",
    "        ax.set_xlabel('Key Position')\n",
    "        ax.set_ylabel('Query Position')\n",
    "        \n",
    "        # Add position labels\n",
    "        ax.set_xticks([0, 1, 2])\n",
    "        ax.set_xticklabels(['a', 'b', '='])\n",
    "        ax.set_yticks([0, 1, 2])\n",
    "        ax.set_yticklabels(['a', 'b', '='])\n",
    "        \n",
    "        plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{EXPERIMENT_DIR}/figures/attention_patterns.png', dpi=200, bbox_inches='tight')\n",
    "print(\"✓ Saved: attention_patterns.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 5: Circuit Similarity Analysis\n",
    "\n",
    "Quantify how similar the learned circuits are across models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import spearmanr\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "def compute_circuit_similarity(neuron_freqs_1, neuron_freqs_2, neuron_frac_1, neuron_frac_2):\n",
    "    \"\"\"\n",
    "    Compute similarity between two circuits based on neuron frequency assignments.\n",
    "    \"\"\"\n",
    "    # Frequency assignment correlation\n",
    "    freq_corr, _ = spearmanr(neuron_freqs_1, neuron_freqs_2)\n",
    "    \n",
    "    # Fraction explained correlation\n",
    "    frac_corr, _ = spearmanr(neuron_frac_1, neuron_frac_2)\n",
    "    \n",
    "    # Frequency distribution similarity (histogram intersection)\n",
    "    unique_freqs = np.union1d(neuron_freqs_1, neuron_freqs_2)\n",
    "    hist1 = np.array([np.sum(neuron_freqs_1 == f) for f in unique_freqs])\n",
    "    hist2 = np.array([np.sum(neuron_freqs_2 == f) for f in unique_freqs])\n",
    "    \n",
    "    # Normalize histograms\n",
    "    hist1 = hist1 / hist1.sum()\n",
    "    hist2 = hist2 / hist2.sum()\n",
    "    \n",
    "    hist_similarity = np.minimum(hist1, hist2).sum()  # Intersection\n",
    "    \n",
    "    return {\n",
    "        'freq_correlation': freq_corr,\n",
    "        'frac_correlation': frac_corr,\n",
    "        'hist_similarity': hist_similarity\n",
    "    }\n",
    "\n",
    "# Compute all pairwise similarities\n",
    "print(\"Computing circuit similarity metrics...\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "similarity_matrix = {}\n",
    "\n",
    "model_keys = ['addition', 'grokked_transfer', 'memorized_transfer', 'random_baseline']\n",
    "\n",
    "for i, key1 in enumerate(model_keys):\n",
    "    for key2 in model_keys[i:]:\n",
    "        sim = compute_circuit_similarity(\n",
    "            neuron_analysis[key1]['freqs'],\n",
    "            neuron_analysis[key2]['freqs'],\n",
    "            neuron_analysis[key1]['frac_explained'],\n",
    "            neuron_analysis[key2]['frac_explained']\n",
    "        )\n",
    "        \n",
    "        similarity_matrix[f\"{key1} vs {key2}\"] = sim\n",
    "        \n",
    "        if key1 != key2:\n",
    "            print(f\"\\n{key1} vs {key2}:\")\n",
    "            print(f\"  Frequency correlation: {sim['freq_correlation']:.3f}\")\n",
    "            print(f\"  Fraction explained correlation: {sim['frac_correlation']:.3f}\")\n",
    "            print(f\"  Histogram similarity: {sim['hist_similarity']:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize similarity matrix\n",
    "model_labels = ['Grokked\\nAddition', 'Grokked\\nTransfer', 'Memorized\\nTransfer', 'Random\\nBaseline']\n",
    "\n",
    "# Create similarity matrices for each metric\n",
    "metrics = ['freq_correlation', 'hist_similarity']\n",
    "metric_names = ['Frequency Correlation', 'Histogram Similarity']\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "for idx, (metric, name) in enumerate(zip(metrics, metric_names)):\n",
    "    # Build symmetric matrix\n",
    "    n = len(model_keys)\n",
    "    sim_matrix = np.zeros((n, n))\n",
    "    \n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            if i == j:\n",
    "                sim_matrix[i, j] = 1.0\n",
    "            elif i < j:\n",
    "                key = f\"{model_keys[i]} vs {model_keys[j]}\"\n",
    "                sim_matrix[i, j] = similarity_matrix[key][metric]\n",
    "                sim_matrix[j, i] = sim_matrix[i, j]\n",
    "            else:\n",
    "                key = f\"{model_keys[j]} vs {model_keys[i]}\"\n",
    "                sim_matrix[i, j] = similarity_matrix[key][metric]\n",
    "    \n",
    "    ax = axes[idx]\n",
    "    im = ax.imshow(sim_matrix, cmap='RdYlGn', vmin=-1 if metric == 'freq_correlation' else 0, vmax=1)\n",
    "    \n",
    "    # Add text annotations\n",
    "    for i in range(n):\n",
    "        for j in range(n):\n",
    "            text = ax.text(j, i, f'{sim_matrix[i, j]:.2f}',\n",
    "                          ha=\"center\", va=\"center\", color=\"black\", fontsize=11, fontweight='bold')\n",
    "    \n",
    "    ax.set_xticks(range(n))\n",
    "    ax.set_yticks(range(n))\n",
    "    ax.set_xticklabels(model_labels, fontsize=10)\n",
    "    ax.set_yticklabels(model_labels, fontsize=10)\n",
    "    ax.set_title(name, fontsize=14, fontweight='bold')\n",
    "    \n",
    "    plt.colorbar(im, ax=ax, fraction=0.046, pad=0.04)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{EXPERIMENT_DIR}/figures/circuit_similarity_matrices.png', dpi=200, bbox_inches='tight')\n",
    "print(\"✓ Saved: circuit_similarity_matrices.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 6: Summary and Interpretation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"CIRCUIT ANALYSIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n1. KEY FINDINGS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Compare grokked transfer to addition\n",
    "sim_grok = similarity_matrix['addition vs grokked_transfer']\n",
    "print(f\"\\nGrokked Addition → Grokked Transfer:\")\n",
    "print(f\"  Frequency correlation: {sim_grok['freq_correlation']:.3f}\")\n",
    "print(f\"  Circuit similarity: {sim_grok['hist_similarity']:.3f}\")\n",
    "if sim_grok['freq_correlation'] > 0.5:\n",
    "    print(f\"  → STRONG CIRCUIT TRANSFER ✓\")\n",
    "else:\n",
    "    print(f\"  → WEAK CIRCUIT TRANSFER\")\n",
    "\n",
    "# Compare memorized transfer to addition\n",
    "sim_mem = similarity_matrix['addition vs memorized_transfer']\n",
    "print(f\"\\nGrokked Addition → Memorized Transfer:\")\n",
    "print(f\"  Frequency correlation: {sim_mem['freq_correlation']:.3f}\")\n",
    "print(f\"  Circuit similarity: {sim_mem['hist_similarity']:.3f}\")\n",
    "if sim_mem['freq_correlation'] > 0.5:\n",
    "    print(f\"  → STRONG CIRCUIT TRANSFER\")\n",
    "elif sim_mem['freq_correlation'] > 0.2:\n",
    "    print(f\"  → MODERATE CIRCUIT TRANSFER\")\n",
    "else:\n",
    "    print(f\"  → WEAK CIRCUIT TRANSFER ✓\")\n",
    "\n",
    "# Compare grokked vs memorized transfer\n",
    "sim_comp = similarity_matrix['grokked_transfer vs memorized_transfer']\n",
    "print(f\"\\nGrokked Transfer vs Memorized Transfer:\")\n",
    "print(f\"  Frequency correlation: {sim_comp['freq_correlation']:.3f}\")\n",
    "print(f\"  Circuit similarity: {sim_comp['hist_similarity']:.3f}\")\n",
    "\n",
    "print(\"\\n2. INTERPRETATION:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "if sim_grok['freq_correlation'] > sim_mem['freq_correlation']:\n",
    "    ratio = sim_grok['freq_correlation'] / (sim_mem['freq_correlation'] + 1e-6)\n",
    "    print(f\"\\n✓ Grokked circuits transfer MORE effectively than memorized circuits\")\n",
    "    print(f\"  Grokked has {ratio:.1f}x stronger correlation with source\")\n",
    "    print(f\"\\n  This confirms that GENERALIZING MECHANISMS transfer, not just any patterns!\")\n",
    "else:\n",
    "    print(f\"\\n⚠ Both grokked and memorized show similar circuit transfer\")\n",
    "    print(f\"  This suggests both conditions learn similar algorithms\")\n",
    "\n",
    "print(\"\\n3. MECHANISTIC INSIGHTS:\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Neuron specialization\n",
    "grok_spec = neuron_analysis['grokked_transfer']['frac_explained'].mean()\n",
    "mem_spec = neuron_analysis['memorized_transfer']['frac_explained'].mean()\n",
    "rand_spec = neuron_analysis['random_baseline']['frac_explained'].mean()\n",
    "\n",
    "print(f\"\\nNeuron specialization (mean fraction variance explained):\")\n",
    "print(f\"  Grokked transfer:   {grok_spec:.3f}\")\n",
    "print(f\"  Memorized transfer: {mem_spec:.3f}\")\n",
    "print(f\"  Random baseline:    {rand_spec:.3f}\")\n",
    "\n",
    "if grok_spec > mem_spec:\n",
    "    print(f\"\\n  → Grokked transfer has MORE specialized neurons\")\n",
    "    print(f\"    (inherited from grokked source model)\")\n",
    "else:\n",
    "    print(f\"\\n  → Similar neuron specialization across conditions\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"ANALYSIS COMPLETE\")\n",
    "print(\"=\"*80)\n",
    "print(f\"\\nFigures saved to: {EXPERIMENT_DIR}/figures/\")\n",
    "print(\"  - fourier_coefficients_comparison.png\")\n",
    "print(\"  - neuron_frequency_specialization.png\")\n",
    "print(\"  - attention_patterns.png\")\n",
    "print(\"  - circuit_similarity_matrices.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This mechanistic analysis reveals **what actually transfers** when using grokked vs memorized models.\n",
    "\n",
    "**Key Questions Answered:**\n",
    "1. Do grokked models transfer their circuit structure? → Check frequency correlations\n",
    "2. Is the transfer different from memorized models? → Compare similarity metrics\n",
    "3. What specific components transfer? → Examine neuron specialization and attention patterns\n",
    "\n",
    "**For the Paper:**\n",
    "Use these visualizations to show that:\n",
    "- Grokked models learn specialized frequency circuits\n",
    "- These circuits transfer to the new task\n",
    "- Memorized models lack this circuit structure\n",
    "- This mechanistically explains the speedup differences!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}