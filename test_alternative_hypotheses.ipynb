{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Testing Alternative Subtraction Hypotheses\n",
    "\n",
    "**Hypothesis 1: Double Negation**\n",
    "- Model computes: `(a - b) = -(-(a) + b) = n - ((n - a) + b)`\n",
    "- More complex: negate a, add b, then negate result\n",
    "- Could explain why it's not using simple negation of b\n",
    "\n",
    "**Hypothesis 2: Bitwise/Encoding Trick**  \n",
    "- Model uses bitwise operations or special encodings\n",
    "- E.g., learns that certain bit patterns correspond to subtraction\n",
    "- Would show up as structured patterns in embeddings\n",
    "\n",
    "**Mathematical Verification:**\n",
    "```\n",
    "Double negation:\n",
    "  -(-(a) + b) mod 113\n",
    "  = (113 - ((113 - a) + b)) mod 113\n",
    "  = (113 - 113 + a - b) mod 113  \n",
    "  = (a - b) mod 113 ✓\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mount Google Drive and navigate to repo\n",
    "from google.colab import drive\n",
    "import os\n",
    "\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "if not os.path.exists('progress-measures-paper-extension'):\n",
    "    !git clone https://github.com/Junekhunter/progress-measures-paper-extension.git\n",
    "\n",
    "os.chdir('progress-measures-paper-extension')\n",
    "!pip install -q einops\n",
    "\n",
    "print(f\"Working directory: {os.getcwd()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import sys\n",
    "sys.path.insert(0, '.')\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from dataclasses import replace\n",
    "from scipy.stats import pearsonr\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "from transformers import Transformer, Config, gen_train_test\n",
    "import helpers\n",
    "\n",
    "sns.set_style('whitegrid')\n",
    "print(\"✓ Imports successful\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "EXPERIMENT_DIR = input(\"Enter experiment directory: \")\n",
    "SEED = 42\n",
    "p = 113\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Device: {device}\")\n",
    "print(f\"Modulus p: {p}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load grokked transfer (subtraction) model\n",
    "print(\"Loading grokked transfer (subtraction) model...\")\n",
    "\n",
    "config = Config(\n",
    "    lr=1e-3,\n",
    "    weight_decay=1.0,\n",
    "    p=p,\n",
    "    d_model=128,\n",
    "    fn_name='subtract',\n",
    "    frac_train=0.3,\n",
    "    seed=SEED,\n",
    "    device=device\n",
    ")\n",
    "\n",
    "checkpoint_path = f'{EXPERIMENT_DIR}/checkpoints/grokked_transfer_seed{SEED}.pth'\n",
    "checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
    "\n",
    "model = Transformer(config, use_cache=False)\n",
    "model.load_state_dict(checkpoint['model_state'], strict=True)\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "print(f\"✓ Model loaded\")\n",
    "print(f\"  Accuracy: {checkpoint['final_test_accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 1: Double Negation Test\n",
    "\n",
    "**Test if model computes**: `n - ((n - a) + b) = a - b`\n",
    "\n",
    "### Three-step process:\n",
    "1. Negate `a`: compute `(n - a)`\n",
    "2. Add `b`: compute `(n - a) + b`  \n",
    "3. Negate result: compute `n - ((n - a) + b) = a - b`\n",
    "\n",
    "### Tests:\n",
    "- **Embedding test**: Check if `embed(a) + embed(n-a) ≈ constant` (negation of a)\n",
    "- **Activation matching**: Compare activations on `(a,b)` vs intermediate steps\n",
    "- **Intervention**: Patch in `(n-a)` and see what happens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"=\"*80)\n",
    "print(\"HYPOTHESIS 1: Double Negation\")\n",
    "print(\"Testing: (a - b) = n - ((n - a) + b)\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Get embedding matrix\n",
    "W_E = model.embed.W_E.data  # [d_model, d_vocab]\n",
    "\n",
    "print(\"\\n1. Testing if model negates 'a' (first input)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Check: does embed(a) + embed(n-a) ≈ constant?\n",
    "negation_sums_a = []\n",
    "for a in range(p):\n",
    "    neg_a = (p - a) % p\n",
    "    sum_vec = (W_E[:, a] + W_E[:, neg_a]).cpu().numpy()\n",
    "    negation_sums_a.append(sum_vec)\n",
    "\n",
    "negation_sums_a = np.array(negation_sums_a)\n",
    "variance_a = negation_sums_a.var(axis=0).mean()\n",
    "baseline_var = W_E.T.cpu().numpy().var(axis=0).mean()\n",
    "\n",
    "reduction_a = variance_a / baseline_var\n",
    "\n",
    "print(f\"Variance of embed(a) + embed(n-a): {variance_a:.6f}\")\n",
    "print(f\"Baseline variance: {baseline_var:.6f}\")\n",
    "print(f\"Reduction ratio: {reduction_a:.4f}\")\n",
    "\n",
    "if reduction_a < 0.1:\n",
    "    print(\"\\n✓ STRONG evidence for negation of 'a'!\")\n",
    "    a_negation_evidence = \"strong\"\n",
    "elif reduction_a < 0.5:\n",
    "    print(\"\\n→ MODERATE evidence for negation of 'a'\")\n",
    "    a_negation_evidence = \"moderate\"\n",
    "else:\n",
    "    print(\"\\n✗ NO evidence for negation of 'a'\")\n",
    "    a_negation_evidence = \"none\"\n",
    "\n",
    "print(\"\\n2. Testing if model negates 'b' (second input)\")\n",
    "print(\"-\" * 80)\n",
    "\n",
    "# Check: does embed(b) + embed(n-b) ≈ constant?\n",
    "negation_sums_b = []\n",
    "for b in range(p):\n",
    "    neg_b = (p - b) % p\n",
    "    sum_vec = (W_E[:, b] + W_E[:, neg_b]).cpu().numpy()\n",
    "    negation_sums_b.append(sum_vec)\n",
    "\n",
    "negation_sums_b = np.array(negation_sums_b)\n",
    "variance_b = negation_sums_b.var(axis=0).mean()\n",
    "reduction_b = variance_b / baseline_var\n",
    "\n",
    "print(f\"Variance of embed(b) + embed(n-b): {variance_b:.6f}\")\n",
    "print(f\"Baseline variance: {baseline_var:.6f}\")\n",
    "print(f\"Reduction ratio: {reduction_b:.4f}\")\n",
    "\n",
    "if reduction_b < 0.1:\n",
    "    print(\"\\n✓ STRONG evidence for negation of 'b'!\")\n",
    "    b_negation_evidence = \"strong\"\n",
    "elif reduction_b < 0.5:\n",
    "    print(\"\\n→ MODERATE evidence for negation of 'b'\")\n",
    "    b_negation_evidence = \"moderate\"\n",
    "else:\n",
    "    print(\"\\n✗ NO evidence for negation of 'b'\")\n",
    "    b_negation_evidence = \"none\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"SUMMARY: Negation Detection\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Negates 'a': {a_negation_evidence} (ratio: {reduction_a:.4f})\")\n",
    "print(f\"Negates 'b': {b_negation_evidence} (ratio: {reduction_b:.4f})\")\n",
    "\n",
    "if a_negation_evidence in [\"strong\", \"moderate\"] and b_negation_evidence == \"none\":\n",
    "    print(\"\\n→ DOUBLE NEGATION hypothesis viable!\")\n",
    "    print(\"  Model may: negate a → add b → negate result\")\n",
    "    double_negation_viable = True\n",
    "elif b_negation_evidence in [\"strong\", \"moderate\"] and a_negation_evidence == \"none\":\n",
    "    print(\"\\n→ SIMPLE NEGATION hypothesis viable!\")\n",
    "    print(\"  Model may: negate b → add a\")\n",
    "    double_negation_viable = False\n",
    "elif a_negation_evidence in [\"strong\", \"moderate\"] and b_negation_evidence in [\"strong\", \"moderate\"]:\n",
    "    print(\"\\n→ BOTH inputs show negation structure!\")\n",
    "    print(\"  Unexpected - may indicate symmetric encoding\")\n",
    "    double_negation_viable = True\n",
    "else:\n",
    "    print(\"\\n→ NO CLEAR negation pattern detected\")\n",
    "    print(\"  Model may use different algorithm entirely\")\n",
    "    double_negation_viable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Compare negation structure for a vs b\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: embed(a) + embed(n-a) heatmap\n",
    "ax = axes[0, 0]\n",
    "im = ax.imshow(negation_sums_a.T, aspect='auto', cmap='RdBu_r')\n",
    "ax.set_xlabel('Token a', fontsize=12)\n",
    "ax.set_ylabel('Embedding Dimension', fontsize=12)\n",
    "ax.set_title('embed(a) + embed(113-a)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Plot 2: embed(b) + embed(n-b) heatmap  \n",
    "ax = axes[0, 1]\n",
    "im = ax.imshow(negation_sums_b.T, aspect='auto', cmap='RdBu_r')\n",
    "ax.set_xlabel('Token b', fontsize=12)\n",
    "ax.set_ylabel('Embedding Dimension', fontsize=12)\n",
    "ax.set_title('embed(b) + embed(113-b)', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax)\n",
    "\n",
    "# Plot 3: Variance per dimension for a\n",
    "ax = axes[1, 0]\n",
    "var_per_dim_a = negation_sums_a.var(axis=0)\n",
    "ax.bar(range(len(var_per_dim_a)), var_per_dim_a, alpha=0.7, color='blue', edgecolor='black')\n",
    "ax.axhline(baseline_var, color='red', linestyle='--', label='Baseline', linewidth=2)\n",
    "ax.set_xlabel('Dimension', fontsize=12)\n",
    "ax.set_ylabel('Variance', fontsize=12)\n",
    "ax.set_title(f'Negation Variance for a (reduction: {reduction_a:.3f})', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "# Plot 4: Variance per dimension for b\n",
    "ax = axes[1, 1]\n",
    "var_per_dim_b = negation_sums_b.var(axis=0)\n",
    "ax.bar(range(len(var_per_dim_b)), var_per_dim_b, alpha=0.7, color='purple', edgecolor='black')\n",
    "ax.axhline(baseline_var, color='red', linestyle='--', label='Baseline', linewidth=2)\n",
    "ax.set_xlabel('Dimension', fontsize=12)\n",
    "ax.set_ylabel('Variance', fontsize=12)\n",
    "ax.set_title(f'Negation Variance for b (reduction: {reduction_b:.3f})', \n",
    "             fontsize=12, fontweight='bold')\n",
    "ax.legend()\n",
    "ax.grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{EXPERIMENT_DIR}/figures/double_negation_test.png', dpi=200, bbox_inches='tight')\n",
    "print(\"✓ Saved: double_negation_test.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hypothesis 2: Bitwise/Encoding Patterns\n",
    "\n",
    "**Test for structured encoding patterns:**\n",
    "- Binary representation correlations\n",
    "- Clustered embeddings (e.g., even/odd, multiples)\n",
    "- Distance preservation (metric structure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "print(\"\\n\" + \"=\"*80)\nprint(\"HYPOTHESIS 2: Bitwise/Encoding Patterns\")\nprint(\"=\"*80)\n\nprint(\"\\n1. Testing Binary Representation Correlation\")\nprint(\"-\" * 80)\n\n# Convert numbers to binary vectors\nnum_bits = int(np.ceil(np.log2(p)))\nbinary_vecs = np.array([[int(b) for b in format(i, f'0{num_bits}b')] for i in range(p)])\n\nprint(f\"Binary representation: {num_bits} bits for p={p}\")\n\n# Get embeddings (only first p tokens, excluding special tokens like \"=\")\nembeddings = W_E[:, :p].T.cpu().numpy()  # [p, d_model]\n\n# For each embedding dimension, compute correlation with each bit\nbit_correlations = []\nfor dim in range(embeddings.shape[1]):\n    embed_dim = embeddings[:, dim]\n    \n    dim_bit_corrs = []\n    for bit in range(num_bits):\n        corr = np.abs(pearsonr(embed_dim, binary_vecs[:, bit])[0])\n        dim_bit_corrs.append(corr)\n    \n    bit_correlations.append(dim_bit_corrs)\n\nbit_correlations = np.array(bit_correlations)  # [d_model, num_bits]\nmax_bit_corr = bit_correlations.max(axis=1).mean()\n\nprint(f\"\\nMean max bit correlation: {max_bit_corr:.3f}\")\n\nif max_bit_corr > 0.7:\n    print(\"✓ STRONG bitwise encoding detected!\")\n    print(\"  Embeddings strongly correlated with binary representation\")\n    bitwise_evidence = \"strong\"\nelif max_bit_corr > 0.4:\n    print(\"→ MODERATE bitwise structure\")\n    bitwise_evidence = \"moderate\"\nelse:\n    print(\"✗ NO significant bitwise encoding\")\n    bitwise_evidence = \"none\"\n\nprint(\"\\n2. Testing Even/Odd Clustering\")\nprint(\"-\" * 80)\n\n# Compute mean embeddings for even vs odd numbers\neven_mask = np.arange(p) % 2 == 0\nodd_mask = ~even_mask\n\nmean_even = embeddings[even_mask].mean(axis=0)\nmean_odd = embeddings[odd_mask].mean(axis=0)\n\n# Distance between even and odd centroids\neven_odd_dist = np.linalg.norm(mean_even - mean_odd)\n\n# Compare to typical distance between random numbers\nrandom_dists = []\nfor _ in range(100):\n    idx1 = np.random.randint(0, p)\n    idx2 = np.random.randint(0, p)\n    dist = np.linalg.norm(embeddings[idx1] - embeddings[idx2])\n    random_dists.append(dist)\n\nmean_random_dist = np.mean(random_dists)\neven_odd_ratio = even_odd_dist / mean_random_dist\n\nprint(f\"Distance between even/odd centroids: {even_odd_dist:.3f}\")\nprint(f\"Mean random distance: {mean_random_dist:.3f}\")\nprint(f\"Ratio: {even_odd_ratio:.3f}\")\n\nif even_odd_ratio > 2.0:\n    print(\"\\n✓ STRONG even/odd clustering!\")\n    clustering_evidence = \"strong\"\nelif even_odd_ratio > 1.3:\n    print(\"\\n→ MODERATE even/odd separation\")\n    clustering_evidence = \"moderate\"\nelse:\n    print(\"\\n✗ NO significant even/odd clustering\")\n    clustering_evidence = \"none\"\n\nprint(\"\\n3. Testing Modular Distance Preservation\")\nprint(\"-\" * 80)\n\n# Check if embedding distance correlates with modular distance\nmod_dists = []\nembed_dists = []\n\nfor i in range(p):\n    for j in range(i+1, min(i+20, p)):  # Sample nearby pairs\n        mod_dist = min((i - j) % p, (j - i) % p)  # Circular distance\n        embed_dist = np.linalg.norm(embeddings[i] - embeddings[j])\n        \n        mod_dists.append(mod_dist)\n        embed_dists.append(embed_dist)\n\ndistance_corr = pearsonr(mod_dists, embed_dists)[0]\n\nprint(f\"Correlation between modular and embedding distance: {distance_corr:.3f}\")\n\nif abs(distance_corr) > 0.7:\n    print(\"\\n✓ STRONG metric structure!\")\n    print(\"  Embeddings preserve modular distance\")\n    metric_evidence = \"strong\"\nelif abs(distance_corr) > 0.4:\n    print(\"\\n→ MODERATE metric structure\")\n    metric_evidence = \"moderate\"\nelse:\n    print(\"\\n✗ NO metric structure preservation\")\n    metric_evidence = \"none\""
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization: Encoding patterns\n",
    "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "# Plot 1: Bit correlation heatmap\n",
    "ax = axes[0, 0]\n",
    "im = ax.imshow(bit_correlations.T, aspect='auto', cmap='YlOrRd', vmin=0, vmax=1)\n",
    "ax.set_xlabel('Embedding Dimension', fontsize=12)\n",
    "ax.set_ylabel('Bit Position', fontsize=12)\n",
    "ax.set_title(f'Binary Encoding Correlation\\n(mean max: {max_bit_corr:.3f})', \n",
    "             fontsize=14, fontweight='bold')\n",
    "plt.colorbar(im, ax=ax, label='|Correlation|')\n",
    "\n",
    "# Plot 2: PCA of embeddings colored by even/odd\n",
    "ax = axes[0, 1]\n",
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "embeddings_2d = pca.fit_transform(embeddings)\n",
    "\n",
    "colors = ['blue' if i % 2 == 0 else 'red' for i in range(p)]\n",
    "ax.scatter(embeddings_2d[:, 0], embeddings_2d[:, 1], c=colors, alpha=0.6, s=20)\n",
    "ax.set_xlabel(f'PC1 ({pca.explained_variance_ratio_[0]:.2%})', fontsize=12)\n",
    "ax.set_ylabel(f'PC2 ({pca.explained_variance_ratio_[1]:.2%})', fontsize=12)\n",
    "ax.set_title(f'PCA: Even (blue) vs Odd (red)\\nSeparation: {even_odd_ratio:.2f}x', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Plot 3: Modular distance vs embedding distance\n",
    "ax = axes[1, 0]\n",
    "ax.scatter(mod_dists, embed_dists, alpha=0.3, s=10)\n",
    "ax.set_xlabel('Modular Distance', fontsize=12)\n",
    "ax.set_ylabel('Embedding Distance', fontsize=12)\n",
    "ax.set_title(f'Distance Preservation\\n(correlation: {distance_corr:.3f})', \n",
    "             fontsize=14, fontweight='bold')\n",
    "ax.grid(True, alpha=0.3)\n",
    "\n",
    "# Add trendline\n",
    "z = np.polyfit(mod_dists, embed_dists, 1)\n",
    "p_line = np.poly1d(z)\n",
    "ax.plot(sorted(mod_dists), p_line(sorted(mod_dists)), \"r--\", alpha=0.8, linewidth=2)\n",
    "\n",
    "# Plot 4: Summary table\n",
    "ax = axes[1, 1]\n",
    "ax.axis('off')\n",
    "\n",
    "summary_text = f\"\"\"\n",
    "ENCODING ANALYSIS SUMMARY\n",
    "{'='*40}\n",
    "\n",
    "Bitwise Encoding:     {bitwise_evidence.upper()}\n",
    "  Mean correlation:   {max_bit_corr:.3f}\n",
    "\n",
    "Even/Odd Clustering:  {clustering_evidence.upper()}\n",
    "  Separation ratio:   {even_odd_ratio:.3f}\n",
    "\n",
    "Metric Structure:     {metric_evidence.upper()}\n",
    "  Distance corr:      {distance_corr:.3f}\n",
    "\n",
    "{'='*40}\n",
    "\"\"\"\n",
    "\n",
    "ax.text(0.1, 0.5, summary_text, fontsize=11, family='monospace',\n",
    "        verticalalignment='center')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(f'{EXPERIMENT_DIR}/figures/encoding_patterns.png', dpi=200, bbox_inches='tight')\n",
    "print(\"✓ Saved: encoding_patterns.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Final Summary: Which Hypothesis is Supported?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"COMPREHENSIVE HYPOTHESIS SUMMARY\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPOTHESIS 1: Double Negation - (a - b) = n - ((n - a) + b)\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Evidence for negating 'a': {a_negation_evidence.upper()} (ratio: {reduction_a:.4f})\")\n",
    "print(f\"Evidence for negating 'b': {b_negation_evidence.upper()} (ratio: {reduction_b:.4f})\")\n",
    "\n",
    "if double_negation_viable:\n",
    "    print(\"\\n✓ DOUBLE NEGATION is VIABLE\")\n",
    "    print(\"  Model shows negation structure in embeddings\")\n",
    "    if a_negation_evidence in [\"strong\", \"moderate\"]:\n",
    "        print(\"  → Suggests: negate(a) → add b → negate result\")\n",
    "else:\n",
    "    print(\"\\n✗ DOUBLE NEGATION not strongly supported\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"HYPOTHESIS 2: Bitwise/Encoding Patterns\")\n",
    "print(\"=\"*80)\n",
    "print(f\"Bitwise encoding:     {bitwise_evidence.upper()} ({max_bit_corr:.3f})\")\n",
    "print(f\"Even/odd clustering:  {clustering_evidence.upper()} ({even_odd_ratio:.3f}x)\")\n",
    "print(f\"Metric preservation:  {metric_evidence.upper()} (r={distance_corr:.3f})\")\n",
    "\n",
    "encoding_score = sum([bitwise_evidence != \"none\", \n",
    "                     clustering_evidence != \"none\",\n",
    "                     metric_evidence != \"none\"])\n",
    "\n",
    "if encoding_score >= 2:\n",
    "    print(\"\\n✓ STRUCTURED ENCODING detected\")\n",
    "    print(\"  Model uses special encoding properties\")\n",
    "elif encoding_score == 1:\n",
    "    print(\"\\n→ SOME encoding structure present\")\n",
    "else:\n",
    "    print(\"\\n✗ NO special encoding detected\")\n",
    "    print(\"  Embeddings appear learned, not structured\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)\n",
    "print(\"OVERALL CONCLUSION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Rank hypotheses\n",
    "hypotheses_ranked = []\n",
    "\n",
    "if b_negation_evidence in [\"strong\", \"moderate\"]:\n",
    "    hypotheses_ranked.append((\"Simple negation: (a + (n-b))\", b_negation_evidence))\n",
    "\n",
    "if double_negation_viable:\n",
    "    hypotheses_ranked.append((\"Double negation: n - ((n-a) + b)\", a_negation_evidence))\n",
    "\n",
    "if encoding_score >= 2:\n",
    "    hypotheses_ranked.append((\"Structured encoding\", \"strong\" if encoding_score == 3 else \"moderate\"))\n",
    "\n",
    "if hypotheses_ranked:\n",
    "    print(\"\\nMost likely mechanisms (ranked):\")\n",
    "    for i, (hyp, strength) in enumerate(hypotheses_ranked, 1):\n",
    "        print(f\"{i}. {hyp} ({strength})\")\n",
    "else:\n",
    "    print(\"\\n→ NO clear mechanism detected\")\n",
    "    print(\"  Model may use a novel algorithm not tested here\")\n",
    "    print(\"  Recommend: Direct intervention experiments or logit lens\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}